{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction 2. Text Extended Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Guillaume Lussier <lussier.guillaume@gmail.com>\n",
    "# base of work http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "# Date: Jan2017\n",
    "# ipython file, kernel 2.7, required modules: sklearn, numpy, pprint, time, logging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the number of vectorizers and classifiers in this document running the whole document can take a minute or two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section4 :  Fitting continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous analysis we used a filtered training set but did not filter the test set, let us compare the results of the classifier on a filtered test set.\n",
    "We use the sklearn fetch_20newsgroups with parameters to remove the headers, footers and quotes from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and filering sklearn.20newsgroup dataset, remove headers, footers, quotes\n",
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n",
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n",
      "Vectorizer on filtered data train group\n",
      "(11314, 101631)\n",
      "Creating Naive Bayes classifier on filtered data group\n",
      "Vectorizer on filtered data test group\n",
      "(7532, 101631)\n",
      "F1 Score on sampled/filtered set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68286112952505695"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# redefine the 20newsgroups dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "# this is to configure python logging to handle warning messages \n",
    "import logging\n",
    "logging.basicConfig()\n",
    "\n",
    "# Categories and Corpus - filtered corpus\n",
    "print(\"Loading and filering sklearn.20newsgroup dataset, remove headers, footers, quotes\")\n",
    "corpus_train_20ng = fetch_20newsgroups(subset='train', shuffle=True, random_state=1, \n",
    "                                    remove=('headers', 'footers', 'quotes'))\n",
    "list_train = list(corpus_train_20ng.target_names)\n",
    "pprint(list_train)\n",
    "\n",
    "# Test Set\n",
    "#filtered test set\n",
    "corpus_test_20ng = fetch_20newsgroups(subset='test', shuffle=True, random_state=1, \n",
    "                                    remove=('headers', 'footers', 'quotes'))\n",
    "# unfiltered test set\n",
    "#corpus_test_20ng = fetch_20newsgroups(subset='test', shuffle=True, random_state=1)\n",
    "list_test = list(corpus_test_20ng.target_names)\n",
    "pprint(list_test)\n",
    "\n",
    "# TF-IDF Vectorizer, filtered corpus\n",
    "print(\"Vectorizer on filtered data train group\")\n",
    "vectorizer_20ng = TfidfVectorizer()\n",
    "vectors = vectorizer_20ng.fit_transform(corpus_train_20ng.data)\n",
    "pprint(vectors.shape)\n",
    "#(11314, 101631)\n",
    "\n",
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on filtered data group\")\n",
    "classifier_20ng = MultinomialNB(alpha=.01)\n",
    "classifier_20ng.fit(vectors, corpus_train_20ng.target)\n",
    "\n",
    "# TF-IDF Vectorizer, test corpus\n",
    "print(\"Vectorizer on filtered data test group\")\n",
    "vectors_test = vectorizer_20ng.transform(corpus_test_20ng.data)\n",
    "pprint(vectors_test.shape)\n",
    "#(7532, 101631)\n",
    "\n",
    "print(\"F1 Score on sampled/filtered set\")\n",
    "predictor = classifier_20ng.predict(vectors_test)\n",
    "metrics.f1_score(corpus_test_20ng.target, predictor, average='macro')\n",
    "#0.68286112952505695 (filtered test set, were headers, footers and quotes have been removed)\n",
    "#0.77414478112872853 (unfiltered test set, same result as in fextraction1 exercise with only train set filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is worst on the filtered test set (no header, footer or quotes) than on the full unfiltered test set. Despite the fact we are using a filtered training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to provide the top 10 features (words) of a category for the provided classifier \n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: not in and it you is that of to the\n",
      "comp.graphics: you in graphics it is for of and to the\n",
      "comp.os.ms-windows.misc: file of you for and is it to windows the\n",
      "comp.sys.ibm.pc.hardware: with scsi for of drive is it and to the\n",
      "comp.sys.mac.hardware: that apple for of mac it and is to the\n",
      "comp.windows.x: for this it in of is and window to the\n",
      "misc.forsale: or in shipping offer 00 to and sale the for\n",
      "rec.autos: is that in it of you and to car the\n",
      "rec.motorcycles: for that in of you it and bike to the\n",
      "rec.sport.baseball: year was is that of in and to he the\n",
      "rec.sport.hockey: hockey team that game of he and in to the\n",
      "sci.crypt: in be it is that key and of to the\n",
      "sci.electronics: that for in it you is and of to the\n",
      "sci.med: this you that in it and is to of the\n",
      "sci.space: for that it is in and space of to the\n",
      "soc.religion.christian: you it in god and is that to of the\n",
      "talk.politics.guns: it gun is you in and that of to the\n",
      "talk.politics.mideast: it is israel that you in and to of the\n",
      "talk.politics.misc: are it is you in and that of to the\n",
      "talk.religion.misc: not it in you is and that to of the\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_20ng, vectorizer_20ng, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen earlier the top 10 features even with a classifier are not necessarily very meaningful. In fextraction1 we have seen how to use a larger top feature set to remove commonalities and improve results. We are going to see below ho to use parameter of the tf-idf vectorization to improve filtering before the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section5 :  Comparing TF-IDF, TF and post-treatment filtering as seen on Sections 3 & 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The post-treatment we have done is centered on removing from the classifier top features the ones that are too common across categories. This is similar to term weighting and inverse document frequency filtering. Let us compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare to a basic CountVectorizer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on filtered data train group\n",
      "(11314, 101631)\n",
      "Creating Naive Bayes classifier on filtered data group\n",
      "Vectorizer on filtered data test group\n",
      "(7532, 101631)\n",
      "F1 Score on sampled/filtered set\n",
      "0.6203806145034193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# TF Vectorizer, filtered corpus\n",
    "print(\"Vectorizer on filtered data train group\")\n",
    "vectorizer_20ng2 = CountVectorizer()\n",
    "vectors2 = vectorizer_20ng2.fit_transform(corpus_train_20ng.data)\n",
    "pprint(vectors2.shape)\n",
    "#(11314, 101631)\n",
    "\n",
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on filtered data group\")\n",
    "classifier_20ng2 = MultinomialNB(alpha=.01)\n",
    "classifier_20ng2.fit(vectors2, corpus_train_20ng.target)\n",
    "\n",
    "# TF Vectorizer, test corpus\n",
    "print(\"Vectorizer on filtered data test group\")\n",
    "vectors_test2 = vectorizer_20ng2.transform(corpus_test_20ng.data)\n",
    "pprint(vectors_test2.shape)\n",
    "#(7532, 101631)\n",
    "\n",
    "print(\"F1 Score on sampled/filtered set\")\n",
    "predictor2 = classifier_20ng2.predict(vectors_test2)\n",
    "pprint(metrics.f1_score(corpus_test_20ng.target, predictor2, average='macro'))\n",
    "#0.6203806145034193 (with count vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector sizes are the same with and without tf-idf, if no parameters are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: not in and it you is that of to the\n",
      "comp.graphics: you in graphics it is for of and to the\n",
      "comp.os.ms-windows.misc: file of you for and is it to windows the\n",
      "comp.sys.ibm.pc.hardware: with scsi for of drive is it and to the\n",
      "comp.sys.mac.hardware: that apple for of mac it and is to the\n",
      "comp.windows.x: for this it in of is and window to the\n",
      "misc.forsale: or in shipping offer 00 to and sale the for\n",
      "rec.autos: is that in it of you and to car the\n",
      "rec.motorcycles: for that in of you it and bike to the\n",
      "rec.sport.baseball: year was is that of in and to he the\n",
      "rec.sport.hockey: hockey team that game of he and in to the\n",
      "sci.crypt: in be it is that key and of to the\n",
      "sci.electronics: that for in it you is and of to the\n",
      "sci.med: this you that in it and is to of the\n",
      "sci.space: for that it is in and space of to the\n",
      "soc.religion.christian: you it in god and is that to of the\n",
      "talk.politics.guns: it gun is you in and that of to the\n",
      "talk.politics.mideast: it is israel that you in and to of the\n",
      "talk.politics.misc: are it is you in and that of to the\n",
      "talk.religion.misc: not it in you is and that to of the\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_20ng, vectorizer_20ng2, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared to tf-idf vectorizer (without parameters)  \n",
    "alt.atheism: not in and it you is that of to the  \n",
    "comp.graphics: you in graphics it is for of and to the  \n",
    "comp.os.ms-windows.misc: file of you for and is it to windows the  \n",
    "comp.sys.ibm.pc.hardware: with scsi for of drive is it and to the  \n",
    "comp.sys.mac.hardware: that apple for of mac it and is to the  \n",
    "comp.windows.x: for this it in of is and window to the  \n",
    "misc.forsale: or in shipping offer 00 to and sale the for  \n",
    "rec.autos: is that in it of you and to car the  \n",
    "rec.motorcycles: for that in of you it and bike to the  \n",
    "rec.sport.baseball: year was is that of in and to he the  \n",
    "rec.sport.hockey: hockey team that game of he and in to the  \n",
    "sci.crypt: in be it is that key and of to the  \n",
    "sci.electronics: that for in it you is and of to the  \n",
    "sci.med: this you that in it and is to of the  \n",
    "sci.space: for that it is in and space of to the  \n",
    "soc.religion.christian: you it in god and is that to of the  \n",
    "talk.politics.guns: it gun is you in and that of to the  \n",
    "talk.politics.mideast: it is israel that you in and to of the  \n",
    "talk.politics.misc: are it is you in and that of to the  \n",
    "talk.religion.misc: not it in you is and that to of the  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on filtered data train group\n",
      "(11314, 39116)\n",
      "Creating Naive Bayes classifier on filtered data group\n",
      "Vectorizer on filtered data test group\n",
      "(7532, 39116)\n",
      "F1 Score on sampled/filtered set\n",
      "0.68007259851926749\n",
      "done in 5.007s.\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorizer with parameter, filtered corpus\n",
    "# here we use the tf-idf vectrizer from sklearn\n",
    "# TERM WEIGHTING\n",
    "# tf / term frequency\n",
    "# idf / inverse documentfrequency\n",
    "# max_df: terms with a frequency higher than this value are ignored\n",
    "# min_df: cut-off, terms wih an obsolute count lower than this value are ignored\n",
    "# analyzer='word': default value, feature will be made of words n-grams\n",
    "# ngram_range=tuple (min_n, max_n) : default 1, n-grams used such as min_n <= n <= max_n\n",
    "# vocabulary: default None, if not given, a vocabulary is determined from the input documents.\n",
    "# max_features: default None, if not None, build a vocabulary with only top max_features ordered by term frequency across the corpus.\n",
    "t0 = time()\n",
    "\n",
    "print(\"Vectorizer on filtered data train group\")\n",
    "vectorizer_20ng3 = TfidfVectorizer(max_df=0.95, min_df=2, \n",
    "                                   stop_words='english')\n",
    "vectors3 = vectorizer_20ng3.fit_transform(corpus_train_20ng.data)\n",
    "pprint(vectors3.shape)\n",
    "#(11314, 39116)\n",
    "# by default 39116 features are extracted with tf-idf and english stop words plus max_df at 95%\n",
    "\n",
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on filtered data group\")\n",
    "classifier_20ng3 = MultinomialNB(alpha=.01)\n",
    "classifier_20ng3.fit(vectors3, corpus_train_20ng.target)\n",
    "\n",
    "# TF-IDF Vectorizer, test corpus\n",
    "print(\"Vectorizer on filtered data test group\")\n",
    "vectors_test3 = vectorizer_20ng3.transform(corpus_test_20ng.data)\n",
    "pprint(vectors_test3.shape)\n",
    "#(7532, 39116)\n",
    "\n",
    "print(\"F1 Score on sampled/filtered set\")\n",
    "predictor3 = classifier_20ng3.predict(vectors_test3)\n",
    "pprint(metrics.f1_score(corpus_test_20ng.target, predictor3, average='macro'))\n",
    "#0.68007259851926749 (with tf-idf & parameters)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#time 5.3s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is slightly improved and wih much smaller vector size (hence memory imprint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: islam atheists say just religion atheism think don people god\n",
      "comp.graphics: format looking 3d know program file files thanks image graphics\n",
      "comp.os.ms-windows.misc: program problem thanks drivers use driver files dos file windows\n",
      "comp.sys.ibm.pc.hardware: monitor disk thanks pc ide controller bus card scsi drive\n",
      "comp.sys.mac.hardware: know monitor quadra does simms problem thanks drive apple mac\n",
      "comp.windows.x: windows xterm x11r5 use application thanks widget motif server window\n",
      "misc.forsale: asking email price sell new condition shipping 00 offer sale\n",
      "rec.autos: don ford new good dealer just engine like cars car\n",
      "rec.motorcycles: don helmet just riding like motorcycle ride bikes dod bike\n",
      "rec.sport.baseball: braves players pitching hit runs games game baseball team year\n",
      "rec.sport.hockey: league year nhl games season players play hockey team game\n",
      "sci.crypt: people use escrow nsa keys government chip clipper encryption key\n",
      "sci.electronics: good voltage thanks used does know like power circuit use\n",
      "sci.med: skepticism cadre dsl banks n3jxp chastity pitt gordon geb msg\n",
      "sci.space: just lunar earth shuttle like moon launch orbit nasa space\n",
      "soc.religion.christian: believe faith christian christ bible people christians church jesus god\n",
      "talk.politics.guns: just law firearms government fbi don weapons people guns gun\n",
      "talk.politics.mideast: jewish arabs arab turkish people armenians armenian jews israeli israel\n",
      "talk.politics.misc: know state president clinton just think tax don government people\n",
      "talk.religion.misc: think don koresh objective christians bible people christian jesus god\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_20ng3, vectorizer_20ng3, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a tf-idf vectorizer with english stop words (removing most common english words) is clearly much effective, F1 score is slightly worse (0.68) than on unfiltered tf-idf (0.77) but the top10 lists show much less overfit with more meaningful words.  \n",
    "The matrix sizes are also much smaller, most common english words have been removed, which brings us from 101631 to 39116 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 max_features impact on tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare the impact of the number of features on the F1 score and fitting (extracted top 10) when using the term frequency and inverse document frequency vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on filtered data train group\n",
      "(11314, 1000)\n",
      "Creating Naive Bayes classifier on filtered data group\n",
      "Vectorizer on filtered data test group\n",
      "(7532, 1000)\n",
      "F1 Score on sampled/filtered set\n",
      "0.50290117598718154\n",
      "done in 4.879s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "print(\"Vectorizer on filtered data train group\")\n",
    "vectorizer_20ng3b = TfidfVectorizer(max_df=0.95, min_df=2, \n",
    "                                   max_features=1000,\n",
    "                                   stop_words='english')\n",
    "vectors3b = vectorizer_20ng3b.fit_transform(corpus_train_20ng.data)\n",
    "pprint(vectors3b.shape)\n",
    "#(11314, 1000)\n",
    "# features extracted are limited to 1000\n",
    "\n",
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on filtered data group\")\n",
    "classifier_20ng3b = MultinomialNB(alpha=.01)\n",
    "classifier_20ng3b.fit(vectors3b, corpus_train_20ng.target)\n",
    "\n",
    "# TF-IDF Vectorizer, test corpus\n",
    "print(\"Vectorizer on filtered data test group\")\n",
    "vectors_test3b = vectorizer_20ng3b.transform(corpus_test_20ng.data)\n",
    "pprint(vectors_test3b.shape)\n",
    "#(7532, 1000)\n",
    "\n",
    "print(\"F1 Score on sampled/filtered set\")\n",
    "predictor3b = classifier_20ng3b.predict(vectors_test3b)\n",
    "pprint(metrics.f1_score(corpus_test_20ng.target, predictor3b, average='macro'))\n",
    "#0.50290117598718154 (with tf-idf and features limited to 1000)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#time 5.2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score is down from 0.68 to 0.5 when we limit the features of the tf-idf vectorization to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: said does say just atheism religion think don people god\n",
      "comp.graphics: does format 3d know program file files thanks image graphics\n",
      "comp.os.ms-windows.misc: using problem driver drivers thanks use files dos file windows\n",
      "comp.sys.ibm.pc.hardware: monitor disk pc thanks ide controller bus scsi card drive\n",
      "comp.sys.mac.hardware: scsi monitor know use does problem thanks drive apple mac\n",
      "comp.windows.x: code program application using thanks use widget motif server window\n",
      "misc.forsale: price email asking sell new condition 00 shipping offer sale\n",
      "rec.autos: think know don new good just engine like cars car\n",
      "rec.motorcycles: think ve good right know don like just dod bike\n",
      "rec.sport.baseball: just good players think hit runs games game team year\n",
      "rec.sport.hockey: think year games nhl players play season hockey team game\n",
      "sci.crypt: escrow people use nsa keys government chip clipper encryption key\n",
      "sci.electronics: ve don good current does know used power like use\n",
      "sci.med: time think like medical people food know don soon edu\n",
      "sci.space: don think shuttle just earth like launch orbit nasa space\n",
      "soc.religion.christian: believe faith bible christian christ christians people church jesus god\n",
      "talk.politics.guns: law just firearms government don fbi weapons people guns gun\n",
      "talk.politics.mideast: turkey just jewish turkish people armenians armenian jews israeli israel\n",
      "talk.politics.misc: did president state clinton think just tax don government people\n",
      "talk.religion.misc: say think just don christians bible christian people jesus god\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_20ng3b, vectorizer_20ng3b, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare this top10 to the previous one we can see more basic words. Because features are limited to 1000, the words that only appeared in one category (less often in the whole documents) have been removed from the features.  \n",
    "We have lost in each document category meaningfulness of our extraction, but we cut from more than 32K features to 1K. The processing time on the other hand has not been reduced significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 max frequency impact on tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous examples we have used a max frequency of 95%. Any word present in more than 95% of the documents was removed from the extracted features. Let us see if it impacts the size and or the fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on filtered data train group\n",
      "(11314, 39116)\n",
      "Creating Naive Bayes classifier on filtered data group\n",
      "Vectorizer on filtered data test group\n",
      "(7532, 39116)\n",
      "F1 Score on sampled/filtered set\n",
      "0.68007259851926749\n",
      "done in 4.782s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "print(\"Vectorizer on filtered data train group\")\n",
    "vectorizer_20ng3c = TfidfVectorizer(min_df=2, \n",
    "                                   stop_words='english')\n",
    "vectors3c = vectorizer_20ng3c.fit_transform(corpus_train_20ng.data)\n",
    "pprint(vectors3c.shape)\n",
    "#(11314, 39116)\n",
    "\n",
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on filtered data group\")\n",
    "classifier_20ng3c = MultinomialNB(alpha=.01)\n",
    "classifier_20ng3c.fit(vectors3c, corpus_train_20ng.target)\n",
    "\n",
    "# TF-IDF Vectorizer, test corpus\n",
    "print(\"Vectorizer on filtered data test group\")\n",
    "vectors_test3c = vectorizer_20ng3c.transform(corpus_test_20ng.data)\n",
    "pprint(vectors_test3c.shape)\n",
    "#(7532, 39116)\n",
    "\n",
    "print(\"F1 Score on sampled/filtered set\")\n",
    "predictor3c = classifier_20ng3c.predict(vectors_test3c)\n",
    "pprint(metrics.f1_score(corpus_test_20ng.target, predictor3c, average='macro'))\n",
    "#0.68007259851926749\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#time 4.8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result comparison for TF-IDF vectorization\n",
    "\n",
    "with max_df = 95%\n",
    "Vectorizer on filtered data train group\n",
    "(11314, 39116)\n",
    "Creating Naive Bayes classifier on filtered data group\n",
    "Vectorizer on filtered data test group\n",
    "(7532, 39116)\n",
    "F1 Score on sampled/filtered set\n",
    "0.68007259851926749\n",
    "done in 4.731s.\n",
    "\n",
    "without maw_df = 95%\n",
    "Vectorizer on filtered data train group\n",
    "(11314, 39116)\n",
    "Creating Naive Bayes classifier on filtered data group\n",
    "Vectorizer on filtered data test group\n",
    "(7532, 39116)\n",
    "F1 Score on sampled/filtered set\n",
    "0.68007259851926749\n",
    "done in 4.779s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: islam atheists say just religion atheism think don people god\n",
      "comp.graphics: format looking 3d know program file files thanks image graphics\n",
      "comp.os.ms-windows.misc: program problem thanks drivers use driver files dos file windows\n",
      "comp.sys.ibm.pc.hardware: monitor disk thanks pc ide controller bus card scsi drive\n",
      "comp.sys.mac.hardware: know monitor quadra does simms problem thanks drive apple mac\n",
      "comp.windows.x: windows xterm x11r5 use application thanks widget motif server window\n",
      "misc.forsale: asking email price sell new condition shipping 00 offer sale\n",
      "rec.autos: don ford new good dealer just engine like cars car\n",
      "rec.motorcycles: don helmet just riding like motorcycle ride bikes dod bike\n",
      "rec.sport.baseball: braves players pitching hit runs games game baseball team year\n",
      "rec.sport.hockey: league year nhl games season players play hockey team game\n",
      "sci.crypt: people use escrow nsa keys government chip clipper encryption key\n",
      "sci.electronics: good voltage thanks used does know like power circuit use\n",
      "sci.med: skepticism cadre dsl banks n3jxp chastity pitt gordon geb msg\n",
      "sci.space: just lunar earth shuttle like moon launch orbit nasa space\n",
      "soc.religion.christian: believe faith christian christ bible people christians church jesus god\n",
      "talk.politics.guns: just law firearms government fbi don weapons people guns gun\n",
      "talk.politics.mideast: jewish arabs arab turkish people armenians armenian jews israeli israel\n",
      "talk.politics.misc: know state president clinton just think tax don government people\n",
      "talk.religion.misc: think don koresh objective christians bible people christian jesus god\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_20ng3c, vectorizer_20ng3c, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features extracted are the same as the tf-idf with 95% max document frequency.  \n",
    "There is no impact of the max document frequency set at 95%, with the common english words already filtered no word has been present in 95% of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 max frequency impact on tf-idf - try stronger filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change the max frequency filtering from 95% to 10% we should see an impact to verify our previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on filtered data train group\n",
      "(11314, 39096)\n",
      "Creating Naive Bayes classifier on filtered data group\n",
      "Vectorizer on filtered data test group\n",
      "(7532, 39096)\n",
      "F1 Score on sampled/filtered set\n",
      "0.67754321082070557\n",
      "done in 4.894s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "print(\"Vectorizer on filtered data train group\")\n",
    "vectorizer_20ng3d = TfidfVectorizer(max_df=0.10, min_df=2, \n",
    "                                   stop_words='english')\n",
    "vectors3d = vectorizer_20ng3d.fit_transform(corpus_train_20ng.data)\n",
    "pprint(vectors3d.shape)\n",
    "#(11314, 39116)\n",
    "\n",
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on filtered data group\")\n",
    "classifier_20ng3d = MultinomialNB(alpha=.01)\n",
    "classifier_20ng3d.fit(vectors3d, corpus_train_20ng.target)\n",
    "\n",
    "# TF-IDF Vectorizer, test corpus\n",
    "print(\"Vectorizer on filtered data test group\")\n",
    "vectors_test3d = vectorizer_20ng3d.transform(corpus_test_20ng.data)\n",
    "pprint(vectors_test3d.shape)\n",
    "#(7532, 39116)\n",
    "\n",
    "print(\"F1 Score on sampled/filtered set\")\n",
    "predictor3d = classifier_20ng3d.predict(vectors_test3d)\n",
    "pprint(metrics.f1_score(corpus_test_20ng.target, predictor3d, average='macro'))\n",
    "#0.68007259851926749\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#time 4.8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result comparison of the max_df parameter impact\n",
    "\n",
    "with max_df = 95%\n",
    "Vectorizer on filtered data train group\n",
    "(11314, 39116)\n",
    "Creating Naive Bayes classifier on filtered data group\n",
    "Vectorizer on filtered data test group\n",
    "(7532, 39116)\n",
    "F1 Score on sampled/filtered set\n",
    "0.68007259851926749\n",
    "done in 4.731s.\n",
    "\n",
    "with max_df = 10%\n",
    "Vectorizer on filtered data train group\n",
    "(11314, 39096)\n",
    "Creating Naive Bayes classifier on filtered data group\n",
    "Vectorizer on filtered data test group\n",
    "(7532, 39096)\n",
    "F1 Score on sampled/filtered set\n",
    "0.67754321082070557\n",
    "done in 4.847s.\n",
    "\n",
    "Only 20 words are common in 10% of all the documents. Removing them doesn't change the results a lot, but we could investigate the efficiency of the max_df parameter if we have not first filtered the english language with the \"english\" parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: moral said objective morality bible islam atheists religion atheism god\n",
      "comp.graphics: hi software format looking 3d program file files image graphics\n",
      "comp.os.ms-windows.misc: using card program problem drivers driver files dos file windows\n",
      "comp.sys.ibm.pc.hardware: drives monitor disk pc ide controller bus card scsi drive\n",
      "comp.sys.mac.hardware: centris scsi lc monitor quadra simms problem drive apple mac\n",
      "comp.windows.x: program using windows xterm x11r5 application widget server motif window\n",
      "misc.forsale: interested asking email price sell condition shipping 00 offer sale\n",
      "rec.autos: toyota drive auto price oil ford dealer engine cars car\n",
      "rec.motorcycles: honda dog bmw helmet riding motorcycle ride bikes dod bike\n",
      "rec.sport.baseball: braves players hit pitching runs games game baseball team year\n",
      "rec.sport.hockey: league year nhl games season players play hockey team game\n",
      "sci.crypt: secure security escrow nsa keys government chip clipper encryption key\n",
      "sci.electronics: work radio amp output ground current electronics voltage power circuit\n",
      "sci.med: skepticism cadre dsl banks n3jxp chastity pitt gordon geb msg\n",
      "sci.space: data spacecraft lunar earth shuttle moon launch orbit nasa space\n",
      "soc.religion.christian: christianity believe faith christian christ bible christians church jesus god\n",
      "talk.politics.guns: weapon police batf firearms law government fbi weapons guns gun\n",
      "talk.politics.mideast: said jewish arabs arab turkish armenians armenian jews israeli israel\n",
      "talk.politics.misc: jobs gay drugs did men state president clinton tax government\n",
      "talk.religion.misc: morality did kent koresh objective christians bible christian jesus god\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_20ng3d, vectorizer_20ng3d, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare this word extraction to the original from the tf-idf:  \n",
    "alt.atheism: islam atheists say just religion atheism think don people god  \n",
    "comp.graphics: looking format 3d know program file files thanks image graphics  \n",
    "comp.os.ms-windows.misc: card problem thanks driver drivers use files dos file windows  \n",
    "comp.sys.ibm.pc.hardware: monitor disk thanks pc ide controller bus card scsi drive  \n",
    "comp.sys.mac.hardware: know monitor does quadra simms thanks problem drive apple mac  \n",
    "comp.windows.x: using windows x11r5 use application thanks widget server motif window  \n",
    "misc.forsale: asking email sell price condition new shipping offer 00 sale  \n",
    "rec.autos: don ford new good dealer just engine like cars car  \n",
    "rec.motorcycles: don just helmet riding like motorcycle ride bikes dod bike  \n",
    "rec.sport.baseball: braves players pitching hit runs games game baseball team year  \n",
    "rec.sport.hockey: league year nhl games season players play hockey team game  \n",
    "sci.crypt: people use escrow nsa keys government chip clipper encryption key  \n",
    "sci.electronics: don thanks voltage used know does like circuit power use  \n",
    "sci.med: skepticism cadre dsl banks chastity n3jxp pitt gordon geb msg  \n",
    "sci.space: just lunar earth shuttle like moon launch orbit nasa space  \n",
    "soc.religion.christian: believe faith christian christ bible people christians church jesus god  \n",
    "talk.politics.guns: just law firearms government fbi don weapons people guns gun  \n",
    "talk.politics.mideast: said arabs arab turkish people armenians armenian jews israeli israel  \n",
    "talk.politics.misc: know state clinton president just think tax don government people  \n",
    "talk.religion.misc: think don koresh objective christians bible people christian jesus god  \n",
    "\n",
    "Let us compare one category:  \n",
    "sci.space: data spacecraft lunar earth shuttle moon launch orbit nasa space  \n",
    "sci.space: just lunar earth shuttle like moon launch orbit nasa space  \n",
    "The differences are:   \n",
    "sci.space: data spacecraft  \n",
    "sci.space: just like  \n",
    "\n",
    "With the extra filtering of document frequency we removed two additional common words: just and like. These two words can have very strong meaning though but for a category like sci.space they can probably be removed without losing information. They were replaced by data and spacecraft, which makes more sense for the sci.space category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 max frequency impact replacing the english words filtering from TFIDFVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can the max frequency be used to replace the english words filtering (stop_words='english')?  \n",
    "Let us compare fitting results from a max_df of 80% to a max_df of 30%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on filtered data train group\n",
      "(11314, 39422)\n",
      "Creating Naive Bayes classifier on filtered data group\n",
      "Vectorizer on filtered data test group\n",
      "(7532, 39422)\n",
      "F1 Score on sampled/filtered set\n",
      "0.68092009218862148\n",
      "done in 4.814s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "print(\"Vectorizer on filtered data train group\")\n",
    "vectorizer_20ng3e = TfidfVectorizer(max_df=0.8, min_df=2)\n",
    "vectors3e = vectorizer_20ng3e.fit_transform(corpus_train_20ng.data)\n",
    "pprint(vectors3e.shape)\n",
    "#(11314, 39422)\n",
    "\n",
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on filtered data group\")\n",
    "classifier_20ng3e = MultinomialNB(alpha=.01)\n",
    "classifier_20ng3e.fit(vectors3e, corpus_train_20ng.target)\n",
    "\n",
    "# TF-IDF Vectorizer, test corpus\n",
    "print(\"Vectorizer on filtered data test group\")\n",
    "vectors_test3e = vectorizer_20ng3e.transform(corpus_test_20ng.data)\n",
    "pprint(vectors_test3e.shape)\n",
    "#(7532, 39422)\n",
    "\n",
    "print(\"F1 Score on sampled/filtered set\")\n",
    "predictor3e = classifier_20ng3e.predict(vectors_test3e)\n",
    "pprint(metrics.f1_score(corpus_test_20ng.target, predictor3e, average='macro'))\n",
    "#0.68092009218862148\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#time 4.8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result comparison for TF-IDF vectorization\n",
    "\n",
    "with english stop words filtering  \n",
    "Vectorizer on filtered data train group\n",
    "(11314, 39116)\n",
    "Creating Naive Bayes classifier on filtered data group\n",
    "Vectorizer on filtered data test group\n",
    "(7532, 39116)\n",
    "F1 Score on sampled/filtered set\n",
    "0.68007259851926749\n",
    "done in 4.731s.\n",
    "\n",
    "with maw_df = 80% and no stop words  \n",
    "Vectorizer on filtered data train group\n",
    "(11314, 39422)\n",
    "Creating Naive Bayes classifier on filtered data group\n",
    "Vectorizer on filtered data test group\n",
    "(7532, 39422)\n",
    "F1 Score on sampled/filtered set\n",
    "0.68092009218862148\n",
    "done in 4.815s.\n",
    "\n",
    "Size of the resulting vectors are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: are not in and it you is that to of\n",
      "comp.graphics: that you in graphics it is for and of to\n",
      "comp.os.ms-windows.misc: in file of you for and is it to windows\n",
      "comp.sys.ibm.pc.hardware: have with scsi for drive of is it and to\n",
      "comp.sys.mac.hardware: with that apple for of mac it and is to\n",
      "comp.windows.x: server for this it in of is and window to\n",
      "misc.forsale: of or in shipping offer 00 to and sale for\n",
      "rec.autos: on is that in it of you and to car\n",
      "rec.motorcycles: my for that in of you it and bike to\n",
      "rec.sport.baseball: they year was is that of in and to he\n",
      "rec.sport.hockey: was hockey team that game of he and in to\n",
      "sci.crypt: this in be it is that key and of to\n",
      "sci.electronics: on that for it in you is and of to\n",
      "sci.med: are this you that in it and is to of\n",
      "sci.space: be for that it is in and space of to\n",
      "soc.religion.christian: not you it in god and is that to of\n",
      "talk.politics.guns: they it gun is you in and that of to\n",
      "talk.politics.mideast: not it is israel you that in and to of\n",
      "talk.politics.misc: are this it you is in and that of to\n",
      "talk.religion.misc: god not it in you is and that to of\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_20ng3e, vectorizer_20ng3e, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features extracted without stop words and a 80% document frequency filtering do not fit the data very well. There are still too many common english words and little of the specificities of each category has been extracted in the top 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on filtered data train group\n",
      "(11314, 39398)\n",
      "Creating Naive Bayes classifier on filtered data group\n",
      "Vectorizer on filtered data test group\n",
      "(7532, 39398)\n",
      "F1 Score on sampled/filtered set\n",
      "0.68088798109027371\n",
      "done in 5.509s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "print(\"Vectorizer on filtered data train group\")\n",
    "vectorizer_20ng3e1 = TfidfVectorizer(max_df=0.3, min_df=2)\n",
    "vectors3e1 = vectorizer_20ng3e1.fit_transform(corpus_train_20ng.data)\n",
    "pprint(vectors3e1.shape)\n",
    "#(11314, 39422)\n",
    "\n",
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on filtered data group\")\n",
    "classifier_20ng3e1 = MultinomialNB(alpha=.01)\n",
    "classifier_20ng3e1.fit(vectors3e1, corpus_train_20ng.target)\n",
    "\n",
    "# TF-IDF Vectorizer, test corpus\n",
    "print(\"Vectorizer on filtered data test group\")\n",
    "vectors_test3e1 = vectorizer_20ng3e1.transform(corpus_test_20ng.data)\n",
    "pprint(vectors_test3e1.shape)\n",
    "#(7532, 39422)\n",
    "\n",
    "print(\"F1 Score on sampled/filtered set\")\n",
    "predictor3e1 = classifier_20ng3e1.predict(vectors_test3e1)\n",
    "pprint(metrics.f1_score(corpus_test_20ng.target, predictor3e1, average='macro'))\n",
    "#0.68092009218862148\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#time 4.8s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result comparison for TF-IDF vectorization\n",
    "\n",
    "with english stop words filtering  \n",
    "Vectorizer on filtered data train group\n",
    "(11314, 39116)\n",
    "Creating Naive Bayes classifier on filtered data group\n",
    "Vectorizer on filtered data test group\n",
    "(7532, 39116)\n",
    "F1 Score on sampled/filtered set\n",
    "0.68007259851926749\n",
    "done in 4.731s.\n",
    "\n",
    "with maw_df = 80% and no stop words  \n",
    "Vectorizer on filtered data train group\n",
    "(11314, 39422)\n",
    "Creating Naive Bayes classifier on filtered data group\n",
    "Vectorizer on filtered data test group\n",
    "(7532, 39422)\n",
    "F1 Score on sampled/filtered set\n",
    "0.68092009218862148\n",
    "done in 4.815s.\n",
    "\n",
    "with maw_df = 30% and no stop words  \n",
    "Vectorizer on filtered data train group\n",
    "(11314, 39398)\n",
    "Creating Naive Bayes classifier on filtered data group\n",
    "Vectorizer on filtered data test group\n",
    "(7532, 39398)\n",
    "F1 Score on sampled/filtered set\n",
    "0.68088798109027371\n",
    "done in 5.109s.\n",
    "\n",
    "Size of the resulting vectors are close, the filtering of the max frequency does not remove many extracted features and the resulting size is close to the stop words filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: one people was so we your they do what god\n",
      "comp.graphics: would me program file files there thanks image any graphics\n",
      "comp.os.ms-windows.misc: thanks use drivers driver there my files dos file windows\n",
      "comp.sys.ibm.pc.hardware: thanks pc any ide controller bus my card scsi drive\n",
      "comp.sys.mac.hardware: there any one problem thanks what my drive apple mac\n",
      "comp.windows.x: use my application do thanks widget any motif server window\n",
      "misc.forsale: price sell please me new condition shipping offer 00 sale\n",
      "rec.autos: me any your out about was cars they my car\n",
      "rec.motorcycles: motorcycle one ride bikes me your dod was my bike\n",
      "rec.sport.baseball: runs games game baseball team his they year was he\n",
      "rec.sport.hockey: nhl season players play they was hockey team game he\n",
      "sci.crypt: nsa would will government keys chip they clipper encryption key\n",
      "sci.electronics: what power circuit anyone use any one they would there\n",
      "sci.med: n3jxp chastity there pitt about gordon geb was msg my\n",
      "sci.space: moon by launch orbit there would they nasa was space\n",
      "soc.religion.christian: they by church who his jesus was he we god\n",
      "talk.politics.guns: what all we were people would guns was they gun\n",
      "talk.politics.mideast: armenians their armenian jews were was by they israeli israel\n",
      "talk.politics.misc: do who would about government what was people we they\n",
      "talk.religion.misc: his by who what your they was jesus he god\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_20ng3e1, vectorizer_20ng3e1, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted features for each category are now closer to the results obtained with the english stop words. The max frequency parameter can be used to filter data the same way as the english stop words, but results will not be as accurate and it could be needed to use a very strong max_df filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 min frequency impact on tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first studied the impact of the max document frequency, let us see how the min document frequency impacts the results of the size of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on filtered data train group\n",
      "(11314, 101323)\n",
      "Creating Naive Bayes classifier on filtered data group\n",
      "Vectorizer on filtered data test group\n",
      "(7532, 101323)\n",
      "F1 Score on sampled/filtered set\n",
      "0.68443899192121638\n",
      "done in 5.820s.\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "print(\"Vectorizer on filtered data train group\")\n",
    "vectorizer_20ng3f = TfidfVectorizer(max_df=0.95, \n",
    "                                   stop_words='english')\n",
    "vectors3f = vectorizer_20ng3f.fit_transform(corpus_train_20ng.data)\n",
    "pprint(vectors3f.shape)\n",
    "#(11314, 101323)\n",
    "\n",
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on filtered data group\")\n",
    "classifier_20ng3f = MultinomialNB(alpha=.01)\n",
    "classifier_20ng3f.fit(vectors3f, corpus_train_20ng.target)\n",
    "\n",
    "# TF-IDF Vectorizer, test corpus\n",
    "print(\"Vectorizer on filtered data test group\")\n",
    "vectors_test3f = vectorizer_20ng3f.transform(corpus_test_20ng.data)\n",
    "pprint(vectors_test3f.shape)\n",
    "#(7532, 101323)\n",
    "\n",
    "print(\"F1 Score on sampled/filtered set\")\n",
    "predictor3f = classifier_20ng3f.predict(vectors_test3f)\n",
    "pprint(metrics.f1_score(corpus_test_20ng.target, predictor3f, average='macro'))\n",
    "#0.68443899192121638\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#time 4.9s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result comparison for TF-IDF vectorization\n",
    "\n",
    "basic results with max_df, min_df and stop words   \n",
    "Vectorizer on filtered data train group\n",
    "(11314, 39116)\n",
    "Creating Naive Bayes classifier on filtered data group\n",
    "Vectorizer on filtered data test group\n",
    "(7532, 39116)\n",
    "F1 Score on sampled/filtered set\n",
    "0.68007259851926749\n",
    "done in 4.731s.\n",
    "\n",
    "without min_df = 2  \n",
    "Vectorizer on filtered data train group\n",
    "(11314, 101323)\n",
    "Creating Naive Bayes classifier on filtered data group\n",
    "Vectorizer on filtered data test group\n",
    "(7532, 101323)\n",
    "F1 Score on sampled/filtered set\n",
    "0.68443899192121638\n",
    "done in 4.884s.\n",
    "\n",
    "F1 scores are close, but the size of the matrixes is much smaller when the rarest words are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: islam atheists say just religion atheism think don people god\n",
      "comp.graphics: looking format 3d know program file files thanks image graphics\n",
      "comp.os.ms-windows.misc: card problem thanks driver drivers use files dos file windows\n",
      "comp.sys.ibm.pc.hardware: monitor disk thanks pc ide controller bus card scsi drive\n",
      "comp.sys.mac.hardware: know monitor does quadra simms thanks problem drive apple mac\n",
      "comp.windows.x: using windows x11r5 use application thanks widget server motif window\n",
      "misc.forsale: asking email sell price condition new shipping offer 00 sale\n",
      "rec.autos: don ford new good dealer just engine like cars car\n",
      "rec.motorcycles: don just helmet riding like motorcycle ride bikes dod bike\n",
      "rec.sport.baseball: braves players pitching hit runs games game baseball team year\n",
      "rec.sport.hockey: league year nhl games season players play hockey team game\n",
      "sci.crypt: people use escrow nsa keys government chip clipper encryption key\n",
      "sci.electronics: don thanks voltage used know does like circuit power use\n",
      "sci.med: skepticism cadre dsl banks chastity n3jxp pitt gordon geb msg\n",
      "sci.space: just lunar earth shuttle like moon launch orbit nasa space\n",
      "soc.religion.christian: believe faith christian christ bible people christians church jesus god\n",
      "talk.politics.guns: just law firearms government fbi don weapons people guns gun\n",
      "talk.politics.mideast: said arabs arab turkish people armenians armenian jews israeli israel\n",
      "talk.politics.misc: know state clinton president just think tax don government people\n",
      "talk.religion.misc: think don koresh objective christians bible people christian jesus god\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_20ng3f, vectorizer_20ng3f, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features extracted fit the categories well without removing the rarest words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What do we  learn from these different results on TF-IDF vectorization and filtering?  \n",
    "1. dictionary filtering using stop_words\n",
    "    This is the most effective to remove less meaningful words and fit the results. It requires having a dictionary for the data filtered though (only English is present in sklearn).\n",
    "    \n",
    "2. max document frequency filtering\n",
    "    This is similar to dictionary filtering aiming at removing the most common (and potentially less meaningful) features. It can require a strong filtering parameter, the F1 scores in our examples were not strongly affected by a max_df as high as 30%.\n",
    "    \n",
    "3. min document frequency filtering\n",
    "    This filtering effect is first to reduce the size of the matrixes. It removes the least frequent words, we will not see an effect on the top 10 extracted features, but it can produce more (false negatives).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section6 : Using other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the previous examples we used a multinomial Naive Bayes (MultinomialNB) classifier. Let us compare results with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new top words that works with models that are not classifiers but matrix decomposition\n",
    "def print_top_n_words(model, vectorizer, corpus, n_top_words):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    categories = corpus.target_names\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d: %s\" % (i, \" \".join([feature_names[j]\n",
    "                        for j in topic.argsort()[:-n_top_words - 1:-1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer used is tf-idf\n",
      "Creating Non Negative Matrix Factorization matrix decomposition\n",
      "done in 11.719s.\n",
      "Topics in Non Negative Matrix Factorization matrix decomposition model\n",
      "Topic 0: don people just like think know time good ve right\n",
      "Topic 1: god believe bible faith truth existence belief hell heaven atheism\n",
      "Topic 2: thanks mail does know advance hi info looking anybody address\n",
      "Topic 3: drive scsi ide disk drives hard controller floppy hd cd\n",
      "Topic 4: 00 sale 50 shipping 20 10 price 15 new 25\n",
      "Topic 5: geb dsl n3jxp chastity cadre shameful pitt intellect skepticism surrender\n",
      "Topic 6: windows dos ms running version os microsoft nt using drivers\n",
      "Topic 7: window manager application motif display server screen xterm widget program\n",
      "Topic 8: game espn games baseball hockey detroit leafs wings night blues\n",
      "Topic 9: car cars dealer engine miles owner buy speed ford tires\n",
      "Topic 10: bobbe ico beauchaine tek queens bronx sank manhattan com blew\n",
      "Topic 11: key keys bit des bits public escrow 80 pgp serial\n",
      "Topic 12: israel israeli jews arab arabs lebanese lebanon peace israelis jewish\n",
      "Topic 13: file files ftp program directory zip image format gif bmp\n",
      "Topic 14: space nasa shuttle launch station sci gov orbit moon lunar\n",
      "Topic 15: card video monitor vga bus cards drivers color driver ram\n",
      "Topic 16: team games players year season hockey play teams nhl league\n",
      "Topic 17: did koresh fbi didn claim objective question children retarded puck\n",
      "Topic 18: jesus christ christian bible christians faith law sin church christianity\n",
      "Topic 19: encryption chip clipper government privacy law escrow algorithm enforcement secure\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# we use vectorizer_20ng3 and vectors3 from previous tf-idf examples\n",
    "print(\"Vectorizer used is tf-idf\")\n",
    "# vectors3\n",
    "#(11314, 39116)\n",
    "# vectors_test3\n",
    "#(7532, 39116)\n",
    "\n",
    "t0 = time()\n",
    "# Fit the Non Negative Matrix Factorization matrix decomposition\n",
    "print(\"Creating Non Negative Matrix Factorization matrix decomposition\")\n",
    "model_nmf = NMF(n_components=20,\n",
    "                random_state=1,\n",
    "                alpha=.1,\n",
    "                l1_ratio=.5)\n",
    "model_nmf.fit(vectors3, corpus_train_20ng.target)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#time 2.738s for 10 components\n",
    "#time 11.662 for 20 components\n",
    "\n",
    "print(\"Topics in Non Negative Matrix Factorization matrix decomposition model\")\n",
    "print_top_n_words(model_nmf, vectorizer_20ng3, corpus_train_20ng, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: time complexity is polynomial for NMF with number of components.  \n",
    "Even with a number of topics equal to the original categories into the 20 newsgroups, the topics extracted are not the same as the categories. It is possible to find similarities like for the sci.space category.  \n",
    "td-idf + multinomial Naive Bayes  \n",
    "sci.space: just lunar earth shuttle like moon launch orbit nasa space  \n",
    "tf-idf + Non Negative Matrix Factorization  \n",
    "Topc 14: space nasa shuttle launch station sci gov orbit moon lunar  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer used is tf-idf\n",
      "Creating Latent Dirichlet Allocation model\n",
      "done in 22.159s.\n",
      "Topics in Latent Dirichlet Allocation model\n",
      "Topic 0: like just know don people think does use thanks good\n",
      "Topic 1: ax ik kr rb oy w1 vv whirrr ky w7\n",
      "Topic 2: intellect geb shameful cadre pitt chastity n3jxp dsl skepticism gordon\n",
      "Topic 3: keller ivy quakers kkeller upenn sas champs xarchie usl ites\n",
      "Topic 4: transoft humanist spreads basalts bensen wingo regolith 3he ppb crust\n",
      "Topic 5: cache ram rear hd automatics miles card anyways simm odometer\n",
      "Topic 6: trc zmed16 amoco sandiego graig nettles 8330 704 phys bickering\n",
      "Topic 7: rectum siberian 712 1261 269 676 70k sputter whooooooooshhhhhh game\n",
      "Topic 8: team game armenian players armenians season turkish games turkey play\n",
      "Topic 9: decnet trademark wimp xdmcp retentive anal sparky helmeted storming survivalist\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# we use vectorizer_20ng3 and vectors3 from previous tf-idf examples\n",
    "print(\"Vectorizer used is tf-idf\")\n",
    "# vectors3\n",
    "#(11314, 39116)\n",
    "# vectors_test3\n",
    "#(7532, 39116)\n",
    "\n",
    "t0 = time()\n",
    "# Fit the Latent Dirichlet Allocation matrix decomposition\n",
    "print(\"Creating Latent Dirichlet Allocation model\")\n",
    "model_lda = LatentDirichletAllocation(n_topics=10,\n",
    "                                      max_iter=5,\n",
    "                                      learning_method='online',\n",
    "                                      learning_offset=50., # tau_0\n",
    "                                      random_state=0)\n",
    "model_lda.fit(vectors3, corpus_train_20ng.target)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#time 21.025s\n",
    "\n",
    "print(\"Topics in Latent Dirichlet Allocation model\")\n",
    "print_top_n_words(model_lda, vectorizer_20ng3, corpus_train_20ng, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: time complexity is proportional to (data_samples * iterations) for LDA\n",
    "The results from the NMF and LDA are pretty different, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The extracted feature words tend to support the use of a 'multinomial Naive Bayes' classifier over the matrix reduction techniques such as NMF or LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
