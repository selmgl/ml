{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction 3. More Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Guillaume Lussier <lussier.guillaume@gmail.com>\n",
    "# base of work http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "# Date: Feb2017\n",
    "# ipython file, kernel 2.7, required modules: sklearn, numpy, pprint, time, logging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section7 : Using other classifiers from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use other classifiers from sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and filering sklearn.20newsgroup dataset, remove headers, footers, quotes\n",
      "Vectorizer tf-idf filtering on data train group\n",
      "(11314, 39116)\n",
      "Creating Naive Bayes classifier on filtered data group\n",
      "Vectorizer tf-idf filtering on data test group\n",
      "(7532, 39116)\n",
      "F1 Score on sampled/filtered set:\n",
      "0.68007259851926749\n",
      "Extracted Top10 Features per Category/Topic:\n",
      "alt.atheism: islam atheists say just religion atheism think don people god\n",
      "comp.graphics: format looking 3d know program file files thanks image graphics\n",
      "comp.os.ms-windows.misc: program problem thanks drivers use driver files dos file windows\n",
      "comp.sys.ibm.pc.hardware: monitor disk thanks pc ide controller bus card scsi drive\n",
      "comp.sys.mac.hardware: know monitor quadra does simms problem thanks drive apple mac\n",
      "comp.windows.x: windows xterm x11r5 use application thanks widget motif server window\n",
      "misc.forsale: asking email price sell new condition shipping 00 offer sale\n",
      "rec.autos: don ford new good dealer just engine like cars car\n",
      "rec.motorcycles: don helmet just riding like motorcycle ride bikes dod bike\n",
      "rec.sport.baseball: braves players pitching hit runs games game baseball team year\n",
      "rec.sport.hockey: league year nhl games season players play hockey team game\n",
      "sci.crypt: people use escrow nsa keys government chip clipper encryption key\n",
      "sci.electronics: good voltage thanks used does know like power circuit use\n",
      "sci.med: skepticism cadre dsl banks n3jxp chastity pitt gordon geb msg\n",
      "sci.space: just lunar earth shuttle like moon launch orbit nasa space\n",
      "soc.religion.christian: believe faith christian christ bible people christians church jesus god\n",
      "talk.politics.guns: just law firearms government fbi don weapons people guns gun\n",
      "talk.politics.mideast: jewish arabs arab turkish people armenians armenian jews israeli israel\n",
      "talk.politics.misc: know state president clinton just think tax don government people\n",
      "talk.religion.misc: think don koresh objective christians bible people christian jesus god\n"
     ]
    }
   ],
   "source": [
    "#redefining the basic sklearn environment for text classification\n",
    "# redefine the 20newsgroups dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "# this is to configure python logging to handle warning messages \n",
    "import logging\n",
    "logging.basicConfig()\n",
    "\n",
    "# Categories and Corpus - filtered corpus\n",
    "print(\"Loading and filering sklearn.20newsgroup dataset, remove headers, footers, quotes\")\n",
    "corpus_train_20ng = fetch_20newsgroups(subset='train', shuffle=True, random_state=1, \n",
    "                                    remove=('headers', 'footers', 'quotes'))\n",
    "list_train = list(corpus_train_20ng.target_names)\n",
    "#pprint(list_train)\n",
    "\n",
    "# Test Set\n",
    "#filtered test set\n",
    "corpus_test_20ng = fetch_20newsgroups(subset='test', shuffle=True, random_state=1, \n",
    "                                    remove=('headers', 'footers', 'quotes'))\n",
    "list_test = list(corpus_test_20ng.target_names)\n",
    "#pprint(list_test)\n",
    "\n",
    "# TF-IDF Vectorizer, filtered corpus\n",
    "print(\"Vectorizer tf-idf filtering on data train group\")\n",
    "vectorizer_20ng = TfidfVectorizer(max_df=0.95, min_df=2, \n",
    "                                   stop_words='english')\n",
    "vectors_train = vectorizer_20ng.fit_transform(corpus_train_20ng.data)\n",
    "pprint(vectors_train.shape)\n",
    "#(11314, 101631)\n",
    "\n",
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on filtered data group\")\n",
    "classifier_20ng_mNB = MultinomialNB(alpha=.01)\n",
    "classifier_20ng_mNB.fit(vectors_train, corpus_train_20ng.target)\n",
    "\n",
    "# TF-IDF Vectorizer, test corpus\n",
    "print(\"Vectorizer tf-idf filtering on data test group\")\n",
    "vectors_test = vectorizer_20ng.transform(corpus_test_20ng.data)\n",
    "pprint(vectors_test.shape)\n",
    "#(7532, 101631)\n",
    "\n",
    "print(\"F1 Score on sampled/filtered set:\")\n",
    "predictor_mNB = classifier_20ng_mNB.predict(vectors_test)\n",
    "pprint(metrics.f1_score(corpus_test_20ng.target, predictor_mNB, average='macro'))\n",
    "#0.68286112952505695 (filtered test set, were headers, footers and quotes have been removed)\n",
    "\n",
    "print(\"Extracted Top10 Features per Category/Topic:\")\n",
    "show_top10(classifier_20ng_mNB, vectorizer_20ng, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to provide the top 10 features (words) of a category for the provided classifier\n",
    "# this works for classifiers with a coef_ attribute\n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Linear Model - Logictic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer used is tf-idf\n",
      "Fiting Random Forest Classifier\n",
      "F1 Score - Logistic Regression\n",
      "0.67388146101858992\n",
      "done in 4.485s.\n",
      "Topics with a Logistic Regression Classifier\n",
      "alt.atheism: punishment atheist motto deletion islamic bobby atheists islam religion atheism\n",
      "comp.graphics: polygon pov cview files tiff format images 3d image graphics\n",
      "comp.os.ms-windows.misc: win3 risc fonts files drivers driver cica ax file windows\n",
      "comp.sys.ibm.pc.hardware: bios 486 monitor drive card ide controller bus pc scsi\n",
      "comp.sys.mac.hardware: nubus powerbook duo simms lc centris se quadra apple mac\n",
      "comp.windows.x: widgets sun application mit x11r5 xterm widget motif server window\n",
      "misc.forsale: new asking email interested 00 condition sell shipping offer sale\n",
      "rec.autos: vw gt auto toyota oil dealer engine ford cars car\n",
      "rec.motorcycles: harley dog bmw riding helmet motorcycle ride bikes dod bike\n",
      "rec.sport.baseball: phillies ball pitching stadium hit cubs runs braves year baseball\n",
      "rec.sport.hockey: puck playoffs leafs players play season nhl team game hockey\n",
      "sci.crypt: privacy crypto security chip keys government nsa encryption clipper key\n",
      "sci.electronics: current amp 8051 output radio ground power voltage electronics circuit\n",
      "sci.med: diet cancer food patients treatment pain medical disease doctor msg\n",
      "sci.space: solar lunar earth shuttle spacecraft moon launch orbit nasa space\n",
      "soc.religion.christian: scripture marriage jesus faith christian christianity christ christians church god\n",
      "talk.politics.guns: jmd firearm nra batf weapon firearms fbi weapons guns gun\n",
      "talk.politics.mideast: jewish arabs turkey armenian turkish arab armenians jews israeli israel\n",
      "talk.politics.misc: jobs gay men people trial president drugs government clinton tax\n",
      "talk.religion.misc: values morality rosicrucian god christians jesus objective kent christian koresh\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# we use vectorizer_20ng3 and vectors3 from previous tf-idf examples\n",
    "print(\"Vectorizer used is tf-idf\")\n",
    "# vectors3\n",
    "#(11314, 39116)\n",
    "# vectors_test3\n",
    "#(7532, 39116)\n",
    "\n",
    "t0 = time()\n",
    "# Fit the Non Negative Matrix Factorization matrix decomposition\n",
    "print(\"Fiting Random Forest Classifier\")\n",
    "logreg = LogisticRegression(random_state=0)\n",
    "logreg.fit(vectors_train, corpus_train_20ng.target)\n",
    "\n",
    "print(\"F1 Score - Logistic Regression\")\n",
    "predictor_lr = logreg.predict(vectors_test)\n",
    "pprint(metrics.f1_score(corpus_test_20ng.target, predictor_lr, average='macro'))\n",
    "#0.67388146101858992\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#time 5.173s\n",
    "\n",
    "print(\"Topics with a Logistic Regression Classifier\")\n",
    "show_top10(logreg, vectorizer_20ng, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare to the multinomial Naive Bayes classifier results:\n",
    "alt.atheism: islam atheists say just religion atheism think don people god\n",
    "comp.graphics: format looking 3d know program file files thanks image graphics\n",
    "comp.os.ms-windows.misc: program problem thanks drivers use driver files dos file windows\n",
    "comp.sys.ibm.pc.hardware: monitor disk thanks pc ide controller bus card scsi drive\n",
    "comp.sys.mac.hardware: know monitor quadra does simms problem thanks drive apple mac\n",
    "comp.windows.x: windows xterm x11r5 use application thanks widget motif server window\n",
    "misc.forsale: asking email price sell new condition shipping 00 offer sale\n",
    "rec.autos: don ford new good dealer just engine like cars car\n",
    "rec.motorcycles: don helmet just riding like motorcycle ride bikes dod bike\n",
    "rec.sport.baseball: braves players pitching hit runs games game baseball team year\n",
    "rec.sport.hockey: league year nhl games season players play hockey team game\n",
    "sci.crypt: people use escrow nsa keys government chip clipper encryption key\n",
    "sci.electronics: good voltage thanks used does know like power circuit use\n",
    "sci.med: skepticism cadre dsl banks n3jxp chastity pitt gordon geb msg\n",
    "sci.space: just lunar earth shuttle like moon launch orbit nasa space\n",
    "soc.religion.christian: believe faith christian christ bible people christians church jesus god\n",
    "talk.politics.guns: just law firearms government fbi don weapons people guns gun\n",
    "talk.politics.mideast: jewish arabs arab turkish people armenians armenian jews israeli israel\n",
    "talk.politics.misc: know state president clinton just think tax don government people\n",
    "talk.religion.misc: think don koresh objective christians bible people christian jesus god\n",
    "\n",
    "Let us use the triage functions defined in fextraction1 to see what are the different words extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# different words in 2 lists, with list comprehension (faster for big lists)\n",
    "def dwl(list1, list2):\n",
    "    return([x for x in (set(list1)^set(list2))])\n",
    "\n",
    "def unique1(list1, list2):\n",
    "    return([x for x in (set(list1)-set(list2))])\n",
    "\n",
    "def unique2(list1, list2):\n",
    "    return([x for x in (set(list2)-set(list1))])\n",
    "\n",
    "# extract unique words from 2 classifier tops10 lists, they have to use the same categories/topics and same vetorizer\n",
    "def unique_top(n, classifier1, classifier2, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        features_top_1 = feature_names[np.argsort(classifier1.coef_[i])[-n:]]\n",
    "        features_top_2 = feature_names[np.argsort(classifier2.coef_[i])[-n:]]\n",
    "        print(\"%s: %s\" % (category, \" \".join(dwl(features_top_1, features_top_2))))\n",
    "\n",
    "def unique1_top(n, classifier1, classifier2, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        features_top_1 = feature_names[np.argsort(classifier1.coef_[i])[-n:]]\n",
    "        features_top_2 = feature_names[np.argsort(classifier2.coef_[i])[-n:]]\n",
    "        print(\"%s: %s\" % (category, \" \".join(unique1(features_top_1, features_top_2))))\n",
    "        \n",
    "def unique2_top(n, classifier1, classifier2, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        features_top_1 = feature_names[np.argsort(classifier1.coef_[i])[-n:]]\n",
    "        features_top_2 = feature_names[np.argsort(classifier2.coef_[i])[-n:]]\n",
    "        print(\"%s: %s\" % (category, \" \".join(unique2(features_top_1, features_top_2))))\n",
    "        \n",
    "# same words in 2 lists, with list comprehension (faster for big lists)\n",
    "def cwl(list1, list2):\n",
    "    return([x for x in list1 if x in set(list2)])\n",
    "\n",
    "# extract same words from 2 classifier tops10 lists, they have to use the same categories/topics and same vetorizer\n",
    "def same_top(n, classifier1, classifier2, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        features_top_1 = feature_names[np.argsort(classifier1.coef_[i])[-n:]]\n",
    "        features_top_2 = feature_names[np.argsort(classifier2.coef_[i])[-n:]]\n",
    "        print(\"%s: %s\" % (category, \" \".join(cwl(features_top_1, features_top_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: don just bobby atheist people punishment islamic say deletion god motto think\n",
      "comp.graphics: polygon pov tiff know looking program thanks file images cview\n",
      "comp.os.ms-windows.misc: use win3 fonts risc cica program thanks dos ax problem\n",
      "comp.sys.ibm.pc.hardware: bios thanks 486 disk\n",
      "comp.sys.mac.hardware: lc know duo thanks powerbook nubus drive centris does problem se monitor\n",
      "comp.windows.x: windows sun use thanks widgets mit\n",
      "misc.forsale: price interested\n",
      "rec.autos: gt don like just auto vw toyota good oil new\n",
      "rec.motorcycles: don like just dog harley bmw\n",
      "rec.sport.baseball: ball stadium phillies players game cubs team games\n",
      "rec.sport.hockey: league playoffs puck games leafs year\n",
      "sci.crypt: use people privacy crypto escrow security\n",
      "sci.electronics: 8051 good like thanks use know current does used radio amp output electronics ground\n",
      "sci.med: pain cancer doctor food medical cadre disease diet pitt geb patients gordon treatment dsl banks n3jxp skepticism chastity\n",
      "sci.space: like just spacecraft solar\n",
      "soc.religion.christian: bible people christianity marriage believe scripture\n",
      "talk.politics.guns: batf don just government nra people firearm law weapon jmd\n",
      "talk.politics.mideast: turkey people\n",
      "talk.politics.misc: men jobs just gay drugs think trial state know don\n",
      "talk.religion.misc: bible people rosicrucian values kent don think morality\n"
     ]
    }
   ],
   "source": [
    "# words unique to Logistic Regression or multinomial Naive Bayes\n",
    "unique_top(10, classifier_20ng_mNB, logreg, vectorizer_20ng, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: islam atheists religion atheism\n",
      "comp.graphics: format 3d files image graphics\n",
      "comp.os.ms-windows.misc: drivers driver files file windows\n",
      "comp.sys.ibm.pc.hardware: monitor pc ide controller bus card scsi drive\n",
      "comp.sys.mac.hardware: quadra simms apple mac\n",
      "comp.windows.x: xterm x11r5 application widget motif server window\n",
      "misc.forsale: asking email sell new condition shipping 00 offer sale\n",
      "rec.autos: ford dealer engine cars car\n",
      "rec.motorcycles: helmet riding motorcycle ride bikes dod bike\n",
      "rec.sport.baseball: braves pitching hit runs baseball year\n",
      "rec.sport.hockey: nhl season players play hockey team game\n",
      "sci.crypt: nsa keys government chip clipper encryption key\n",
      "sci.electronics: voltage power circuit\n",
      "sci.med: msg\n",
      "sci.space: lunar earth shuttle moon launch orbit nasa space\n",
      "soc.religion.christian: faith christian christ christians church jesus god\n",
      "talk.politics.guns: firearms fbi weapons guns gun\n",
      "talk.politics.mideast: jewish arabs arab turkish armenians armenian jews israeli israel\n",
      "talk.politics.misc: president clinton tax government people\n",
      "talk.religion.misc: koresh objective christians christian jesus god\n"
     ]
    }
   ],
   "source": [
    "# words common to Logistic Regression or multinomial Naive Bayes\n",
    "same_top(10, classifier_20ng_mNB, logreg, vectorizer_20ng, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results of the top 10 features extracted by Logistic Regression and multinomial Naive Bayes are pretty different.  \n",
    "The common words are clearly most central features to be extracted from each category.  \n",
    "Let us compare unique results from both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Topics with a multinomial Naive Bayes:\n",
      "alt.atheism: don just people god say think\n",
      "comp.graphics: looking program thanks file know\n",
      "comp.os.ms-windows.misc: dos use program thanks problem\n",
      "comp.sys.ibm.pc.hardware: disk thanks\n",
      "comp.sys.mac.hardware: monitor drive does thanks problem know\n",
      "comp.windows.x: windows use thanks\n",
      "misc.forsale: price\n",
      "rec.autos: new good don like just\n",
      "rec.motorcycles: don like just\n",
      "rec.sport.baseball: players game games team\n",
      "rec.sport.hockey: league games year\n",
      "sci.crypt: use escrow people\n",
      "sci.electronics: use good like used does thanks know\n",
      "sci.med: n3jxp pitt geb dsl chastity banks cadre skepticism gordon\n",
      "sci.space: like just\n",
      "soc.religion.christian: believe bible people\n",
      "talk.politics.guns: law people don just government\n",
      "talk.politics.mideast: people\n",
      "talk.politics.misc: state don know just think\n",
      "talk.religion.misc: bible don think people\n",
      "\n",
      "Unique Topics with a Logistic Regression Classifier:\n",
      "alt.atheism: bobby atheist islamic deletion motto punishment\n",
      "comp.graphics: pov tiff images polygon cview\n",
      "comp.os.ms-windows.misc: win3 fonts cica risc ax\n",
      "comp.sys.ibm.pc.hardware: bios 486\n",
      "comp.sys.mac.hardware: lc duo powerbook nubus centris se\n",
      "comp.windows.x: widgets sun mit\n",
      "misc.forsale: interested\n",
      "rec.autos: auto gt toyota oil vw\n",
      "rec.motorcycles: dog bmw harley\n",
      "rec.sport.baseball: phillies ball cubs stadium\n",
      "rec.sport.hockey: puck playoffs leafs\n",
      "sci.crypt: security crypto privacy\n",
      "sci.electronics: 8051 current radio amp output electronics ground\n",
      "sci.med: pain cancer doctor food medical disease diet patients treatment\n",
      "sci.space: spacecraft solar\n",
      "soc.religion.christian: scripture marriage christianity\n",
      "talk.politics.guns: batf nra weapon jmd firearm\n",
      "talk.politics.mideast: turkey\n",
      "talk.politics.misc: drugs trial men jobs gay\n",
      "talk.religion.misc: rosicrucian values kent morality\n"
     ]
    }
   ],
   "source": [
    "# words unique to multinomial Naive Bayes\n",
    "print(\"Unique Topics with a multinomial Naive Bayes:\")\n",
    "unique1_top(10, classifier_20ng_mNB, logreg, vectorizer_20ng, corpus_train_20ng.target_names)\n",
    "print(\"\")\n",
    "# words unique to Logistic Regression\n",
    "print(\"Unique Topics with a Logistic Regression Classifier:\")\n",
    "unique2_top(10, classifier_20ng_mNB, logreg, vectorizer_20ng, corpus_train_20ng.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at unique words from each classifier the Logistic Regression results look more meaningful for each category than the multinomial Naive Bayes.  \n",
    "Considering the F1 scores are very close this would push towards using Logistic Regression for this feature extraction problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Ensemble Method - Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer used is tf-idf\n",
      "Fiting Random Forest Classifier\n",
      "F1 Score - Random Forest Classifier\n",
      "0.60335367250287308\n",
      "done in 11.575s.\n",
      "Topics with a Random Forest Classifier\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# we use vectorizer_20ng3 and vectors3 from previous tf-idf examples\n",
    "print(\"Vectorizer used is tf-idf\")\n",
    "# vectors3\n",
    "#(11314, 39116)\n",
    "# vectors_test3\n",
    "#(7532, 39116)\n",
    "\n",
    "t0 = time()\n",
    "# Fit the Random Forest classifier\n",
    "print(\"Fiting Random Forest Classifier\")\n",
    "random_forest = RandomForestClassifier(n_estimators=100, # number of trees in the forest\n",
    "                                       max_features=\"auto\", # auto, sqrt, log2\n",
    "                                       max_depth=None, # integer or None\n",
    "                                       min_samples_split=20, # minimum number of samples required to split an internal node\n",
    "                                       n_jobs=-1, # number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores\n",
    "                                       random_state=0) # seed used by the random number generator\n",
    "random_forest.fit(vectors_train, corpus_train_20ng.target)\n",
    "\n",
    "print(\"F1 Score - Random Forest Classifier\")\n",
    "predictor_rf = random_forest.predict(vectors_test)\n",
    "pprint(metrics.f1_score(corpus_test_20ng.target, predictor_rf, average='macro'))\n",
    "#\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#\n",
    "\n",
    "print(\"Topics with a Random Forest Classifier\")\n",
    "#show_top10(random_forest, vectorizer_20ng, corpus_train_20ng.target_names)\n",
    "#print_top_n_words(random_forest, vectorizer_20ng, corpus_train_20ng, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact of number of estimators (trees):  \n",
    "1. n_estimators=10, min_samples_split=2  \n",
    "F1 Score : 0.50706915419130261  \n",
    "done in 3.562s.\n",
    "2. n_estimators=100, min_samples_split=2  \n",
    "F1 Score : 0.59718046613774967\n",
    "done in 29.829s.\n",
    "3. n_estimators=1, min_samples_split=2  \n",
    "F1 Score : 0.31805565386545032  \n",
    "done in 0.911s.  \n",
    "\n",
    "The runtime returned for calculation of the classifier is linear with the number of estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact of minimum number of splits :  \n",
    "1. n_estimators=10, min_samples_split=2  \n",
    "F1 Score : 0.50706915419130261  \n",
    "done in 3.562s.\n",
    "2. n_estimators=10, min_samples_split=20  \n",
    "F1 Score : 0.54781375149316192  \n",
    "done in 1.586s.\n",
    "3. n_estimators=10, min_samples_split=5  \n",
    "F1 Score : 0.53908788926598206   \n",
    "done in 2.411s.  \n",
    "\n",
    "Increasing the minimum splits decreases calculation time and improves F1 Score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we combine a higher number of estimators with a larger minimum splits:  \n",
    "n_estimators=100, min_samples_split=20  \n",
    "F1 Score - Random Forest: 0.60335367250287308     \n",
    "done in 10.589s.  \n",
    "The results are better but still inferior to Logictic Regression below:  \n",
    "F1 Score - Logistic Regression : 0.67388146101858992  \n",
    "done in 4.939s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the matrix models in fextraction2, the random forest classifier in sklearn library sklearn.ensemble.RandomForestClassifier does not have a coef_ attribute. We cannot use the show_top10 function to display the top extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to provide the top 10 features from a random forest\n",
    "def randomtrees_top_n(classifier, vectorizer, categories, n):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    top_n = np.argsort(random_forest.feature_importances_)[-n:]\n",
    "    print(\"Top %d: %s\" % (n, \" \".join(feature_names[top_n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20L\n",
      "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19])\n",
      "39116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([30791, 16466,  6974,  8308, 37783], dtype=int64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint(random_forest.n_classes_)\n",
    "pprint(random_forest.classes_)\n",
    "pprint(random_forest.n_features_)\n",
    "np.argsort(random_forest.feature_importances_)[-5:]\n",
    "# [34358,  9259, 16901, 22034, 32570, 30791, 16466,  6974,  8308, 37783]\n",
    "# [30791, 16466,  6974,  8308, 37783]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10: team clipper gun mac space sale god bike car windows\n"
     ]
    }
   ],
   "source": [
    "randomtrees_top_n(random_forest, vectorizer_20ng, corpus_train_20ng.target_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The radom forest top 10 are all present in the common words extracted by the Logistic Regression and multinomial Naive Bayes:  \n",
    "alt.atheism: islam atheists religion atheism  \n",
    "comp.graphics: format 3d files image graphics  \n",
    "comp.os.ms-windows.misc: drivers driver files file **windows**  \n",
    "comp.sys.ibm.pc.hardware: monitor pc ide controller bus card scsi drive  \n",
    "comp.sys.mac.hardware: quadra simms apple **mac**  \n",
    "comp.windows.x: xterm x11r5 application widget motif server window  \n",
    "misc.forsale: asking email sell new condition shipping 00 offer **sale**  \n",
    "rec.autos: ford dealer engine cars **car**  \n",
    "rec.motorcycles: helmet riding motorcycle ride bikes dod **bike**  \n",
    "rec.sport.baseball: braves pitching hit runs baseball year  \n",
    "rec.sport.hockey: nhl season players play hockey **team** game  \n",
    "sci.crypt: nsa keys government chip **clipper** encryption key  \n",
    "sci.electronics: voltage power circuit  \n",
    "sci.med: msg  \n",
    "sci.space: lunar earth shuttle moon launch orbit nasa **space**  \n",
    "soc.religion.christian: faith christian christ christians church jesus god  \n",
    "talk.politics.guns: firearms fbi weapons guns **gun**  \n",
    "talk.politics.mideast: jewish arabs arab turkish armenians armenian jews israeli israel  \n",
    "talk.politics.misc: president clinton tax government people  \n",
    "talk.religion.misc: koresh objective christians christian jesus **god**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest was effective in extracting the top words over the whole 20newsgroups data. The extracted data are not matched to the original categories as they are for the Logistic Regression or multinomial Naive Bayes.  \n",
    "Depending of which result is needed (overall data or per category) one type of classifier will be easier to use directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do: in fextraction4 we will discuss the use of multiple base estimators/classifiers to improve generalizability / robustness over a single estimator. (to be noted that random forest is already an ensemble method that achieves this result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
