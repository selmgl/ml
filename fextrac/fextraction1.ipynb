{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction 1. Text Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Guillaume Lussier <lussier.guillaume@gmail.com>\n",
    "# base of work http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "# Date: Jan2017\n",
    "# ipython file, kernel 2.7, required modules: sklearn, numpy, pprint, time, logging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section1 :  scikitlearn Extraction from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 1 comes from scikit-learn documentation\n",
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "# CountVectorizer implements both tokenization and occurrence counting in a single class\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer\n",
    "\n",
    "#CountVectorizer(analyzer=...'word', binary=False, decode_error=...'strict',\n",
    "#        dtype=<... 'numpy.int64'>, encoding=...'utf-8', input=...'content',\n",
    "#        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "#        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
    "#        strip_accents=None, token_pattern=...'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "#        tokenizer=None, vocabulary=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data to analyze and the features extracted\n",
    "corpus = ['This is the first document.', 'This is the second second document.', 'And the third one.', 'Is this the first document?']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X\n",
    "#<4x9 sparse matrix of type '<... 'numpy.int64'>'\n",
    "#    with 19 stored elements in Compressed Sparse ... format>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of analysis\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\") == (['this', 'is', 'text', 'document', 'to', 'analyze'])\n",
    "#True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the list of features\n",
    "vectorizer.get_feature_names() == (['and', 'document', 'first', 'is', 'one','second', 'the', 'third', 'this'])\n",
    "#True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data matrix\n",
    "X.toarray()           \n",
    "#array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
    "#       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
    "#       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
    "#       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('document')\n",
    "#1 'document' is part of the vocabulary learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counter-example, text unknown to our analyzer\n",
    "vectorizer.transform(['Something completely new.']).toarray()\n",
    "#array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...) these three words are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################################\n",
    "# BI-GRAMS\n",
    "# analyzing single words losses the ordering, so working on pairs can help\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!') == (['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\n",
    "#True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X_2\n",
    "#array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
    "#       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
    "#       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "#       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_index = bigram_vectorizer.vocabulary_.get('is this')\n",
    "X_2[:, feature_index]\n",
    "#array([0, 0, 0, 1]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=False, sublinear_tf=False,\n",
       "         use_idf=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################################\n",
    "# TERM WEIGHTING\n",
    "# tf / term frequency\n",
    "# idf / inverse documentfrequency\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer   \n",
    "#TfidfTransformer(norm=...'l2', smooth_idf=False, sublinear_tf=False,\n",
    "#                 use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [[3, 0, 1], [2, 0, 0], [3, 0, 0], [4, 0, 0], [3, 2, 0], [3, 0, 2]]\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf                         \n",
    "#<6x3 sparse matrix of type '<... 'numpy.float64'>'\n",
    "#    with 9 stored elements in Compressed Sparse ... format>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81940995,  0.        ,  0.57320793],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.47330339,  0.88089948,  0.        ],\n",
       "       [ 0.58149261,  0.        ,  0.81355169]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()                        \n",
    "#array([[ 0.81940995,  0.        ,  0.57320793],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 0.47330339,  0.88089948,  0.        ],\n",
    "#       [ 0.58149261,  0.        ,  0.81355169]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85151335,  0.        ,  0.52433293],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.55422893,  0.83236428,  0.        ],\n",
       "       [ 0.63035731,  0.        ,  0.77630514]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformer.fit_transform(counts).toarray()\n",
    "#array([[ 0.85151335,  0.        ,  0.52433293],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 0.55422893,  0.83236428,  0.        ],\n",
    "#       [ 0.63035731,  0.        ,  0.77630514]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  2.25276297,  1.84729786])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.idf_                       \n",
    "#array([ 1. ...,  2.25...,  1.84...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combining CountVectorizer and TfidfTransformer in a single model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "vectorizer.fit_transform(corpus)\n",
    "#<4x9 sparse matrix of type '<... 'numpy.float64'>'\n",
    "#    with 19 stored elements in Compressed Sparse ... format>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################################\n",
    "# Performance Improvement - HASHING\n",
    "# Vectorization of large text corpus\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "hv.transform(corpus)\n",
    "#<4x10 sparse matrix of type '<... 'numpy.float64'>'\n",
    "#    with 16 stored elements in Compressed Sparse ... format>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section2 : some few more basic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'is', 'one', 'text', 'this', 'three', 'two']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_a = CountVectorizer(min_df=1)\n",
    "corpus_ex = ['this is a text o one','this is an text two','this is text three']\n",
    "X_a = vectorizer_a.fit_transform(corpus_ex)\n",
    "vectorizer_a.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_a.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'text', 'document', 'to', 'analyze']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_a = vectorizer_a.build_analyzer()\n",
    "analyze_a(\"This is a text document to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'text', 'contains', 'one', 'two', 'three']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_a(\"This text contains one, two, three.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'is', 'one', 'text', 'this', 'three', 'two']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_b = TfidfVectorizer(min_df=1)\n",
    "X_b = vectorizer_b.fit_transform(corpus_ex)\n",
    "vectorizer_b.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 : Application sklearn.20newsgroups dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the scikitlearn capabilities above on a real dataset preloaded in scikitlearn: the 20newsgroups dataset  \n",
    "\"The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation).\"\n",
    "\n",
    "20newgroups info can be found on http://scikit-learn.org/stable/datasets/ section 5.8\n",
    "The goal is to study the impact of the data filtering by comparing results on:\n",
    "1. full text set of 20newsgroup\n",
    "2. removing headers, footers and quotes\n",
    "3. limiting the data set to smaller sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorization from sklearn.datasets.20newsgroups\n",
    "# 20newgroups info can be found on http://scikit-learn.org/stable/datasets/ section 5.8\n",
    "# goal is to study the impact of the data filtering by comparing results on \n",
    "# 1. full text set of 20newsgroup\n",
    "# 2. removing headers, footers and quotes\n",
    "# 3. limiting the data set to smaller sizes\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "# this is to configure python logging to handle warning messages \n",
    "import logging\n",
    "logging.basicConfig()\n",
    "\n",
    "# configuration parameters\n",
    "fetch20_corpus_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sklearn.datasets.twenty_newsgroups:Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sklearn.20newsgroup dataset\n",
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n",
      "done in 86.062s.\n"
     ]
    }
   ],
   "source": [
    "# Categories and Corpus - full training set\n",
    "print(\"Loading sklearn.20newsgroup dataset\")\n",
    "t0 = time()\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_train_categories = list(newsgroups_train.target_names)\n",
    "pprint(newsgroups_train_categories)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#done in 0.5..s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# corresponding test set\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=newsgroups_train_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and filering sklearn.20newsgroup dataset, remove headers, footers, quotes\n",
      "done in 1.243s.\n"
     ]
    }
   ],
   "source": [
    "# Categories and Corpus - filtered corpus\n",
    "print(\"Loading and filering sklearn.20newsgroup dataset, remove headers, footers, quotes\")\n",
    "t0 = time()\n",
    "corpus_fetch20 = fetch_20newsgroups(subset='train', shuffle=True, random_state=1, \n",
    "                                    categories= newsgroups_train_categories,\n",
    "                                    remove=('headers', 'footers', 'quotes'))\n",
    "#corpus_samples = corpus_fetch20.data[:fetch20_corpus_size]\n",
    "corpus_samples = corpus_fetch20.data\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#done in 2.3..s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on full data *train* group\n",
      "(11314, 130107)\n",
      "done in 3.298s.\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer, full corpus\n",
    "print(\"Vectorizer on full data *train* group\")\n",
    "t0 = time()\n",
    "vectorizer_fetch20_full = TfidfVectorizer()\n",
    "vectors_full = vectorizer_fetch20_full.fit_transform(newsgroups_train.data)\n",
    "pprint(vectors_full.shape)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#(11314, 130107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on sampled data *train* group\n",
      "(11314, 101631)\n",
      "done in 2.151s.\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer, filtered corpus\n",
    "print(\"Vectorizer on sampled data *train* group\")\n",
    "t0 = time()\n",
    "vectorizer_fetch20_samples = TfidfVectorizer()\n",
    "vectors_samples = vectorizer_fetch20_samples.fit_transform(corpus_samples)\n",
    "pprint(vectors_samples.shape)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#(100, 5647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Naive Bayes classifier on full data group\n",
      "done in 0.181s.\n",
      "Vectorizer on full data -test- group\n",
      "(7532, 130107)\n",
      "done in 1.971s.\n"
     ]
    }
   ],
   "source": [
    "# Classifier 'multinomial Naive Bayes' on full 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on full data group\")\n",
    "t0 = time()\n",
    "classifier_fetch20_full = MultinomialNB(alpha=.01)\n",
    "classifier_fetch20_full.fit(vectors_full, newsgroups_train.target)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# test vector for classifier evaluation\n",
    "print(\"Vectorizer on full data -test- group\")\n",
    "t0 = time()\n",
    "vectors_test_full = vectorizer_fetch20_full.transform(newsgroups_test.data)\n",
    "pprint(vectors_test_full.shape)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#(7532, 130107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on full set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82906596444740432"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 score for full set \n",
    "print(\"F1 Score on full set\")\n",
    "pred_full = classifier_fetch20_full.predict(vectors_test_full)\n",
    "metrics.f1_score(newsgroups_test.target, pred_full, average='macro')\n",
    "#0.82906596444740432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Naive Bayes classifier on sampled data group\n",
      "done in 0.108s.\n",
      "Vectorizer on sampled data -test- group\n",
      "(7532, 101631)\n",
      "done in 2.008s.\n"
     ]
    }
   ],
   "source": [
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on sampled data group\")\n",
    "t0 = time()\n",
    "classifier_fetch20_samples = MultinomialNB(alpha=.01)\n",
    "#classifier_fetch20_samples.fit(vectors_samples, corpus_fetch20.target[:fetch20_corpus_size])\n",
    "classifier_fetch20_samples.fit(vectors_samples, corpus_fetch20.target)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# test vector for classifier evaluation\n",
    "print(\"Vectorizer on sampled data -test- group\")\n",
    "t0 = time()\n",
    "vectors_test_samples = vectorizer_fetch20_samples.transform(newsgroups_test.data)\n",
    "pprint(vectors_test_samples.shape)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#(7532, 5647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on sampled/fltered set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77414478112872853"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 score for full set \n",
    "print(\"F1 Score on sampled/fltered set\")\n",
    "pred_samples = classifier_fetch20_samples.predict(vectors_test_samples)\n",
    "metrics.f1_score(newsgroups_test.target, pred_samples, average='macro')\n",
    "#0.2410889603950605"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 Result Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comparison on the 20newsgroups results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some example for table display, requires tabletext module\n",
    "#import tabletext\n",
    "#table_example = [[1,2,30], [4,23125,6], [7,8,999]]\n",
    "#print tabletext.to_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading Time, Vectorizer Train, Classifier NB, Vectorizer Test\n",
    "results_times_fullset=[0.732, 5.145, 0.217, 2.667]\n",
    "results_times_filteredset=[2.452, 3.568, 0.165, 2.683]\n",
    "results_times_filteredset100=[2.296, 0.070, 0.006, 2.751]\n",
    "\n",
    "#Vectorizer Train, Vectorizer Test\n",
    "results_sizes_fullset=[[11314, 130107], [7532, 130107]]\n",
    "results_sizes_filteredset=[[11314, 101631], [7532, 101631]]\n",
    "results_sizes_filteredset100=[[100, 5647], [7532, 5647]]\n",
    "\n",
    "#F1 scores\n",
    "results_f1_fullset=[0.829]\n",
    "results_f1_filteredset=[0.774]\n",
    "results_f1_filteredset100=[0.241]\n",
    "# filtering doesn't lose to much F1 Score, but reducing the data set to 100 elements brings a strong degadation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top10 representation, most informative features, also used to evaluate overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: keith it and you in that is to of the\n",
      "comp.graphics: edu in for it is and graphics of to the\n",
      "comp.os.ms-windows.misc: file for of and edu is it to the windows\n",
      "comp.sys.ibm.pc.hardware: card ide is of it drive and scsi to the\n",
      "comp.sys.mac.hardware: in it is and of edu apple mac to the\n",
      "comp.windows.x: it mit in motif and is of window to the\n",
      "misc.forsale: shipping offer of 00 to and edu the for sale\n",
      "rec.autos: that is you it in of and to car the\n",
      "rec.motorcycles: dod you it com in of and bike to the\n",
      "rec.sport.baseball: that is baseball and of in to he edu the\n",
      "rec.sport.hockey: ca game he team and hockey of in to the\n",
      "sci.crypt: chip that encryption is and clipper key of to the\n",
      "sci.electronics: for edu you it in is and of to the\n",
      "sci.med: edu pitt that it in and is to of the\n",
      "sci.space: it that is nasa in and to of space the\n",
      "soc.religion.christian: we it in and is god that to of the\n",
      "talk.politics.guns: it is you that gun and in of to the\n",
      "talk.politics.mideast: is you israeli that israel in and to of the\n",
      "talk.politics.misc: edu it is you and in that of to the\n",
      "talk.religion.misc: sandvik god you in is that and to of the\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_fetch20_full, vectorizer_fetch20_full, newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: not in and it you is that of to the\n",
      "comp.graphics: you in graphics it is for of and to the\n",
      "comp.os.ms-windows.misc: file of you for and is it to windows the\n",
      "comp.sys.ibm.pc.hardware: with scsi for of drive is it and to the\n",
      "comp.sys.mac.hardware: that apple for of mac it and is to the\n",
      "comp.windows.x: for this it in of is and window to the\n",
      "misc.forsale: or in shipping offer 00 to and sale the for\n",
      "rec.autos: is that in it of you and to car the\n",
      "rec.motorcycles: for that in of you it and bike to the\n",
      "rec.sport.baseball: year was is that of in and to he the\n",
      "rec.sport.hockey: hockey team that game of he and in to the\n",
      "sci.crypt: in be it is that key and of to the\n",
      "sci.electronics: that for in it you is and of to the\n",
      "sci.med: this you that in it and is to of the\n",
      "sci.space: for that it is in and space of to the\n",
      "soc.religion.christian: you it in god and is that to of the\n",
      "talk.politics.guns: it gun is you in and that of to the\n",
      "talk.politics.mideast: it is israel that you in and to of the\n",
      "talk.politics.misc: are it is you in and that of to the\n",
      "talk.religion.misc: not it in you is and that to of the\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_fetch20_samples, vectorizer_fetch20_samples, corpus_fetch20.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "next to do:\n",
    "1. study overfit vs. performance vs. f1 scores\n",
    "2. work with tf-idf to improve results"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
