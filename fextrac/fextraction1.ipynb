{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction 1. Text Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Guillaume Lussier <lussier.guillaume@gmail.com>\n",
    "# base of work http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "# Date: Jan2017\n",
    "# ipython file, kernel 2.7, required modules: sklearn, numpy, pprint, time, logging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section1 :  scikitlearn Extraction from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 1 comes from scikit-learn documentation\n",
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "# CountVectorizer implements both tokenization and occurrence counting in a single class\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer\n",
    "\n",
    "#CountVectorizer(analyzer=...'word', binary=False, decode_error=...'strict',\n",
    "#        dtype=<... 'numpy.int64'>, encoding=...'utf-8', input=...'content',\n",
    "#        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "#        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
    "#        strip_accents=None, token_pattern=...'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "#        tokenizer=None, vocabulary=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data to analyze and the features extracted\n",
    "corpus = ['This is the first document.', 'This is the second second document.', 'And the third one.', 'Is this the first document?']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X\n",
    "#<4x9 sparse matrix of type '<... 'numpy.int64'>'\n",
    "#    with 19 stored elements in Compressed Sparse ... format>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of analysis\n",
    "analyze = vectorizer.build_analyzer()\n",
    "analyze(\"This is a text document to analyze.\") == (['this', 'is', 'text', 'document', 'to', 'analyze'])\n",
    "#True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the list of features\n",
    "vectorizer.get_feature_names() == (['and', 'document', 'first', 'is', 'one','second', 'the', 'third', 'this'])\n",
    "#True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data matrix\n",
    "X.toarray()           \n",
    "#array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
    "#       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
    "#       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
    "#       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('document')\n",
    "#1 'document' is part of the vocabulary learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counter-example, text unknown to our analyzer\n",
    "vectorizer.transform(['Something completely new.']).toarray()\n",
    "#array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...) these three words are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################################\n",
    "# BI-GRAMS\n",
    "# analyzing single words losses the ordering, so working on pairs can help\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!') == (['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\n",
    "#True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X_2\n",
    "#array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
    "#       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
    "#       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "#       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_index = bigram_vectorizer.vocabulary_.get('is this')\n",
    "X_2[:, feature_index]\n",
    "#array([0, 0, 0, 1]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm=u'l2', smooth_idf=False, sublinear_tf=False,\n",
       "         use_idf=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################################\n",
    "# TERM WEIGHTING\n",
    "# tf / term frequency\n",
    "# idf / inverse documentfrequency\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer   \n",
    "#TfidfTransformer(norm=...'l2', smooth_idf=False, sublinear_tf=False,\n",
    "#                 use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x3 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [[3, 0, 1], [2, 0, 0], [3, 0, 0], [4, 0, 0], [3, 2, 0], [3, 0, 2]]\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf                         \n",
    "#<6x3 sparse matrix of type '<... 'numpy.float64'>'\n",
    "#    with 9 stored elements in Compressed Sparse ... format>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81940995,  0.        ,  0.57320793],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.47330339,  0.88089948,  0.        ],\n",
       "       [ 0.58149261,  0.        ,  0.81355169]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()                        \n",
    "#array([[ 0.81940995,  0.        ,  0.57320793],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 0.47330339,  0.88089948,  0.        ],\n",
    "#       [ 0.58149261,  0.        ,  0.81355169]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85151335,  0.        ,  0.52433293],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.55422893,  0.83236428,  0.        ],\n",
       "       [ 0.63035731,  0.        ,  0.77630514]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformer.fit_transform(counts).toarray()\n",
    "#array([[ 0.85151335,  0.        ,  0.52433293],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 1.        ,  0.        ,  0.        ],\n",
    "#       [ 0.55422893,  0.83236428,  0.        ],\n",
    "#       [ 0.63035731,  0.        ,  0.77630514]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  2.25276297,  1.84729786])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.idf_                       \n",
    "#array([ 1. ...,  2.25...,  1.84...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combining CountVectorizer and TfidfTransformer in a single model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "vectorizer.fit_transform(corpus)\n",
    "#<4x9 sparse matrix of type '<... 'numpy.float64'>'\n",
    "#    with 19 stored elements in Compressed Sparse ... format>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x10 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 16 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################################################\n",
    "# Performance Improvement - HASHING\n",
    "# Vectorization of large text corpus\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "hv.transform(corpus)\n",
    "#<4x10 sparse matrix of type '<... 'numpy.float64'>'\n",
    "#    with 16 stored elements in Compressed Sparse ... format>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section2 : some few additional basic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'an', u'is', u'one', u'text', u'this', u'three', u'two']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_a = CountVectorizer(min_df=1)\n",
    "corpus_ex = ['this is a text o one','this is an text two','this is text three']\n",
    "X_a = vectorizer_a.fit_transform(corpus_ex)\n",
    "vectorizer_a.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_a.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'this', u'is', u'text', u'document', u'to', u'analyze']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_a = vectorizer_a.build_analyzer()\n",
    "analyze_a(\"This is a text document to analyze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'this', u'text', u'contains', u'one', u'two', u'three']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_a(\"This text contains one, two, three.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'an', u'is', u'one', u'text', u'this', u'three', u'two']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_b = TfidfVectorizer(min_df=1)\n",
    "X_b = vectorizer_b.fit_transform(corpus_ex)\n",
    "vectorizer_b.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 : Application sklearn.20newsgroups dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the scikitlearn capabilities above on a real dataset preloaded in scikitlearn: the 20newsgroups dataset  \n",
    "\"The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation).\"\n",
    "\n",
    "20newgroups info can be found on http://scikit-learn.org/stable/datasets/ section 5.8\n",
    "The goal is to study the impact of the data filtering by comparing results on:\n",
    "1. full text set of 20newsgroup\n",
    "2. removing headers, footers and quotes\n",
    "3. limiting the data set to smaller sizes (limited work in this file, see fextraction2 for more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorization from sklearn.datasets.20newsgroups\n",
    "# 20newgroups info can be found on http://scikit-learn.org/stable/datasets/ section 5.8\n",
    "# goal is to study the impact of the data filtering by comparing results on \n",
    "# 1. full text set of 20newsgroup\n",
    "# 2. removing headers, footers and quotes\n",
    "# 3. limiting the data set to smaller sizes\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "# this is to configure python logging to handle warning messages \n",
    "import logging\n",
    "logging.basicConfig()\n",
    "\n",
    "# configuration parameters, used to reduce the size of the corpus for training\n",
    "fetch20_corpus_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sklearn.20newsgroup dataset\n",
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n",
      "done in 0.652s.\n"
     ]
    }
   ],
   "source": [
    "# Categories and Corpus - full training set\n",
    "print(\"Loading sklearn.20newsgroup dataset\")\n",
    "t0 = time()\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_train_categories = list(newsgroups_train.target_names)\n",
    "pprint(newsgroups_train_categories)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#done in 0.5..s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# corresponding test set\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=newsgroups_train_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and filering sklearn.20newsgroup dataset, remove headers, footers, quotes\n",
      "done in 2.335s.\n"
     ]
    }
   ],
   "source": [
    "# Categories and Corpus - filtered corpus\n",
    "print(\"Loading and filering sklearn.20newsgroup dataset, remove headers, footers, quotes\")\n",
    "t0 = time()\n",
    "corpus_fetch20 = fetch_20newsgroups(subset='train', shuffle=True, random_state=1, \n",
    "                                    categories= newsgroups_train_categories,\n",
    "                                    remove=('headers', 'footers', 'quotes'))\n",
    "# for a smaller corpus use the line below\n",
    "#corpus_samples = corpus_fetch20.data[:fetch20_corpus_size]\n",
    "# full filtered corpus\n",
    "corpus_samples = corpus_fetch20.data\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#done in 2.3..s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on full data *train* group\n",
      "(11314, 130107)\n",
      "done in 4.921s.\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer, full corpus\n",
    "print(\"Vectorizer on full data *train* group\")\n",
    "t0 = time()\n",
    "vectorizer_fetch20_full = TfidfVectorizer()\n",
    "vectors_full = vectorizer_fetch20_full.fit_transform(newsgroups_train.data)\n",
    "pprint(vectors_full.shape)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#(11314, 130107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer on sampled data *train* group\n",
      "(11314, 101631)\n",
      "done in 3.188s.\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer, filtered corpus\n",
    "print(\"Vectorizer on sampled data *train* group\")\n",
    "t0 = time()\n",
    "vectorizer_fetch20_samples = TfidfVectorizer()\n",
    "vectors_samples = vectorizer_fetch20_samples.fit_transform(corpus_samples)\n",
    "pprint(vectors_samples.shape)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#(100, 5647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Naive Bayes classifier on full data group\n",
      "done in 0.210s.\n",
      "Vectorizer on full data -test- group\n",
      "(7532, 130107)\n",
      "done in 2.745s.\n"
     ]
    }
   ],
   "source": [
    "# Classifier 'multinomial Naive Bayes' on full 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on full data group\")\n",
    "t0 = time()\n",
    "classifier_fetch20_full = MultinomialNB(alpha=.01)\n",
    "classifier_fetch20_full.fit(vectors_full, newsgroups_train.target)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# test vector for classifier evaluation\n",
    "print(\"Vectorizer on full data -test- group\")\n",
    "t0 = time()\n",
    "vectors_test_full = vectorizer_fetch20_full.transform(newsgroups_test.data)\n",
    "pprint(vectors_test_full.shape)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#(7532, 130107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on full set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82906596444740432"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 score for full set \n",
    "print(\"F1 Score on full set\")\n",
    "pred_full = classifier_fetch20_full.predict(vectors_test_full)\n",
    "metrics.f1_score(newsgroups_test.target, pred_full, average='macro')\n",
    "#0.82906596444740432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Naive Bayes classifier on sampled data group\n",
      "done in 0.165s.\n",
      "Vectorizer on sampled data -test- group\n",
      "(7532, 101631)\n",
      "done in 2.592s.\n"
     ]
    }
   ],
   "source": [
    "# Classifier 'multinomial Naive Bayes' on trimed filtered corpus 20newsgroups set\n",
    "print(\"Creating Naive Bayes classifier on sampled data group\")\n",
    "t0 = time()\n",
    "classifier_fetch20_samples = MultinomialNB(alpha=.01)\n",
    "# for a smaller corpus use the line below\n",
    "#classifier_fetch20_samples.fit(vectors_samples, corpus_fetch20.target[:fetch20_corpus_size])\n",
    "classifier_fetch20_samples.fit(vectors_samples, corpus_fetch20.target)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# test vector for classifier evaluation\n",
    "print(\"Vectorizer on sampled data -test- group\")\n",
    "t0 = time()\n",
    "vectors_test_samples = vectorizer_fetch20_samples.transform(newsgroups_test.data)\n",
    "pprint(vectors_test_samples.shape)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "#(7532, 5647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on sampled/filtered set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77414478112872853"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 score for filtered set \n",
    "print(\"F1 Score on sampled/filtered set\")\n",
    "pred_samples = classifier_fetch20_samples.predict(vectors_test_samples)\n",
    "metrics.f1_score(newsgroups_test.target, pred_samples, average='macro')\n",
    "#0.2410889603950605 for 100 samples\n",
    "#0.77414478112872853 for full filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 Result Comparison and Looking at Top10s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comparison on the 20newsgroups results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some example for table display, requires tabletext module\n",
    "#import tabletext\n",
    "#table_example = [[1,2,30], [4,23125,6], [7,8,999]]\n",
    "#print tabletext.to_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading Time, Vectorizer Train, Classifier NB, Vectorizer Test\n",
    "results_times_fullset=[0.732, 5.145, 0.217, 2.667]\n",
    "results_times_filteredset=[2.452, 3.568, 0.165, 2.683]\n",
    "results_times_filteredset100=[2.296, 0.070, 0.006, 2.751]\n",
    "\n",
    "#Vectorizer Train, Vectorizer Test\n",
    "results_sizes_fullset=[[11314, 130107], [7532, 130107]]\n",
    "results_sizes_filteredset=[[11314, 101631], [7532, 101631]]\n",
    "results_sizes_filteredset100=[[100, 5647], [7532, 5647]]\n",
    "\n",
    "#F1 scores\n",
    "results_f1_fullset=[0.829]\n",
    "results_f1_filteredset=[0.774]\n",
    "results_f1_filteredset100=[0.241]\n",
    "# filtering doesn't lose to much F1 Score, but reducing the data set to 100 elements brings a strong degadation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the F1 scores of our 3 classifiers tested on the test set, the complete set has a 0.83 score, the filtered complete set has a 0.77 score, the filtered strongly reduced set only a 0.24 score. It should be mentioned the reduced set is less than 1% of the size of the full training set.\n",
    "\n",
    "The calculation times for the strongly reduced set are also much smaller, about 50 times less than the full filtered set for the classifier and vectorizer on the training sets of both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note about F1-score (or F-score)\n",
    "F1 score is a measure of the estimator accuracy, it considers both the precison p and recall r to compute the score. It varies between 0 (worst) and 1 (best).  \n",
    "- precision, p: number of correct positive results (true positive, TP) divided by the number of all positive results  (true positive + false positive, TP + FP)  \n",
    "- recall, r: number of correct positive results (true positive, TP) divided by the number of results that should be positive (true positive + false negative, TP + FN)  \n",
    "\n",
    "Another classic measure is Accuracy:\n",
    "- accuracy, acc: number of correct positive results (true positive, TP) added to the number of correct negative results (true negative, TN) divided by the total population (all results, TP+FP+TN+FN) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top10 representation, most informative features, also used to evaluate overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to provide the top 10 features (words) of a category for the provided classifier \n",
    "def show_top10(classifier, vectorizer, categories):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categories):\n",
    "        top10 = np.argsort(classifier.coef_[i])[-10:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: keith it and you in that is to of the\n",
      "comp.graphics: edu in for it is and graphics of to the\n",
      "comp.os.ms-windows.misc: file for of and edu is it to the windows\n",
      "comp.sys.ibm.pc.hardware: card ide is of it drive and scsi to the\n",
      "comp.sys.mac.hardware: in it is and of edu apple mac to the\n",
      "comp.windows.x: it mit in motif and is of window to the\n",
      "misc.forsale: shipping offer of 00 to and edu the for sale\n",
      "rec.autos: that is you it in of and to car the\n",
      "rec.motorcycles: dod you it com in of and bike to the\n",
      "rec.sport.baseball: that is baseball and of in to he edu the\n",
      "rec.sport.hockey: ca game he team and hockey of in to the\n",
      "sci.crypt: chip that encryption is and clipper key of to the\n",
      "sci.electronics: for edu you it in is and of to the\n",
      "sci.med: edu pitt that it in and is to of the\n",
      "sci.space: it that is nasa in and to of space the\n",
      "soc.religion.christian: we it in and is god that to of the\n",
      "talk.politics.guns: it is you that gun and in of to the\n",
      "talk.politics.mideast: is you israeli that israel in and to of the\n",
      "talk.politics.misc: edu it is you and in that of to the\n",
      "talk.religion.misc: sandvik god you in is that and to of the\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_fetch20_full, vectorizer_fetch20_full, newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: not in and it you is that of to the\n",
      "comp.graphics: you in graphics it is for of and to the\n",
      "comp.os.ms-windows.misc: file of you for and is it to windows the\n",
      "comp.sys.ibm.pc.hardware: with scsi for of drive is it and to the\n",
      "comp.sys.mac.hardware: that apple for of mac it and is to the\n",
      "comp.windows.x: for this it in of is and window to the\n",
      "misc.forsale: or in shipping offer 00 to and sale the for\n",
      "rec.autos: is that in it of you and to car the\n",
      "rec.motorcycles: for that in of you it and bike to the\n",
      "rec.sport.baseball: year was is that of in and to he the\n",
      "rec.sport.hockey: hockey team that game of he and in to the\n",
      "sci.crypt: in be it is that key and of to the\n",
      "sci.electronics: that for in it you is and of to the\n",
      "sci.med: this you that in it and is to of the\n",
      "sci.space: for that it is in and space of to the\n",
      "soc.religion.christian: you it in god and is that to of the\n",
      "talk.politics.guns: it gun is you in and that of to the\n",
      "talk.politics.mideast: it is israel that you in and to of the\n",
      "talk.politics.misc: are it is you in and that of to the\n",
      "talk.religion.misc: not it in you is and that to of the\n"
     ]
    }
   ],
   "source": [
    "show_top10(classifier_fetch20_samples, vectorizer_fetch20_samples, corpus_fetch20.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# common words in 2 lists, keeping list1 ordering\n",
    "def cwl1(list1, list2):\n",
    "    list3 = set(list1)&set(list2) # list3 doesn't need to be a list itself\n",
    "    list4 = sorted(list3, key = lambda k : list1.index(k))\n",
    "    return list4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'two']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1= [\"one\", \"two\", \"three\"]\n",
    "list2= [\"two\", \"one\", \"four\"]\n",
    "list3=cwl1(list1, list2)\n",
    "list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cwl with list comprehension (faster for big lists)\n",
    "# both implementation keep ordering from list1\n",
    "def cwl(list1, list2):\n",
    "    s =set(list2)\n",
    "    list3 = [x for x in list1 if x in s]\n",
    "    return(list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one', 'two']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1= [\"one\", \"two\", \"three\"]\n",
    "list2= [\"two\", \"one\", \"four\"]\n",
    "list3=cwl(list1, list2)\n",
    "list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to extract common features (words) for the same category with two classifiers\n",
    "# cat1 and cat2 should be the same or this will not work\n",
    "# cwl is used as feature_names lists do not have an attribute index used in cwl1\n",
    "def common_top10(class1, vect1, cat1, class2, vect2):\n",
    "    feature_names_1 = np.asarray(vect1.get_feature_names())\n",
    "    feature_names_2 = np.asarray(vect2.get_feature_names())\n",
    "    for i, category in enumerate(cat1):\n",
    "        top10_1 = np.argsort(class1.coef_[i])[-10:]\n",
    "        top10_2 = np.argsort(class2.coef_[i])[-10:]\n",
    "        print(\"%s: %s: %s\" % (\"Set1 \", category, \" \".join(feature_names_1[top10_1])))\n",
    "        print(\"%s: %s: %s\" % (\"Set2 \", category, \" \".join(feature_names_2[top10_2])))\n",
    "        print(\"%s: %s: %s\" % (\"Commons \", category, \" \".join(cwl(feature_names_1[top10_1], feature_names_2[top10_2]))))\n",
    "        print(\"%s\" % (\" \")) # just for skipping a line between each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set1 : alt.atheism: keith it and you in that is to of the\n",
      "Set2 : alt.atheism: not in and it you is that of to the\n",
      "Commons : alt.atheism: it and you in that is to of the\n",
      " \n",
      "Set1 : comp.graphics: edu in for it is and graphics of to the\n",
      "Set2 : comp.graphics: you in graphics it is for of and to the\n",
      "Commons : comp.graphics: in for it is and graphics of to the\n",
      " \n",
      "Set1 : comp.os.ms-windows.misc: file for of and edu is it to the windows\n",
      "Set2 : comp.os.ms-windows.misc: file of you for and is it to windows the\n",
      "Commons : comp.os.ms-windows.misc: file for of and is it to the windows\n",
      " \n",
      "Set1 : comp.sys.ibm.pc.hardware: card ide is of it drive and scsi to the\n",
      "Set2 : comp.sys.ibm.pc.hardware: with scsi for of drive is it and to the\n",
      "Commons : comp.sys.ibm.pc.hardware: is of it drive and scsi to the\n",
      " \n",
      "Set1 : comp.sys.mac.hardware: in it is and of edu apple mac to the\n",
      "Set2 : comp.sys.mac.hardware: that apple for of mac it and is to the\n",
      "Commons : comp.sys.mac.hardware: it is and of apple mac to the\n",
      " \n",
      "Set1 : comp.windows.x: it mit in motif and is of window to the\n",
      "Set2 : comp.windows.x: for this it in of is and window to the\n",
      "Commons : comp.windows.x: it in and is of window to the\n",
      " \n",
      "Set1 : misc.forsale: shipping offer of 00 to and edu the for sale\n",
      "Set2 : misc.forsale: or in shipping offer 00 to and sale the for\n",
      "Commons : misc.forsale: shipping offer 00 to and the for sale\n",
      " \n",
      "Set1 : rec.autos: that is you it in of and to car the\n",
      "Set2 : rec.autos: is that in it of you and to car the\n",
      "Commons : rec.autos: that is you it in of and to car the\n",
      " \n",
      "Set1 : rec.motorcycles: dod you it com in of and bike to the\n",
      "Set2 : rec.motorcycles: for that in of you it and bike to the\n",
      "Commons : rec.motorcycles: you it in of and bike to the\n",
      " \n",
      "Set1 : rec.sport.baseball: that is baseball and of in to he edu the\n",
      "Set2 : rec.sport.baseball: year was is that of in and to he the\n",
      "Commons : rec.sport.baseball: that is and of in to he the\n",
      " \n",
      "Set1 : rec.sport.hockey: ca game he team and hockey of in to the\n",
      "Set2 : rec.sport.hockey: hockey team that game of he and in to the\n",
      "Commons : rec.sport.hockey: game he team and hockey of in to the\n",
      " \n",
      "Set1 : sci.crypt: chip that encryption is and clipper key of to the\n",
      "Set2 : sci.crypt: in be it is that key and of to the\n",
      "Commons : sci.crypt: that is and key of to the\n",
      " \n",
      "Set1 : sci.electronics: for edu you it in is and of to the\n",
      "Set2 : sci.electronics: that for in it you is and of to the\n",
      "Commons : sci.electronics: for you it in is and of to the\n",
      " \n",
      "Set1 : sci.med: edu pitt that it in and is to of the\n",
      "Set2 : sci.med: this you that in it and is to of the\n",
      "Commons : sci.med: that it in and is to of the\n",
      " \n",
      "Set1 : sci.space: it that is nasa in and to of space the\n",
      "Set2 : sci.space: for that it is in and space of to the\n",
      "Commons : sci.space: it that is in and to of space the\n",
      " \n",
      "Set1 : soc.religion.christian: we it in and is god that to of the\n",
      "Set2 : soc.religion.christian: you it in god and is that to of the\n",
      "Commons : soc.religion.christian: it in and is god that to of the\n",
      " \n",
      "Set1 : talk.politics.guns: it is you that gun and in of to the\n",
      "Set2 : talk.politics.guns: it gun is you in and that of to the\n",
      "Commons : talk.politics.guns: it is you that gun and in of to the\n",
      " \n",
      "Set1 : talk.politics.mideast: is you israeli that israel in and to of the\n",
      "Set2 : talk.politics.mideast: it is israel that you in and to of the\n",
      "Commons : talk.politics.mideast: is you that israel in and to of the\n",
      " \n",
      "Set1 : talk.politics.misc: edu it is you and in that of to the\n",
      "Set2 : talk.politics.misc: are it is you in and that of to the\n",
      "Commons : talk.politics.misc: it is you and in that of to the\n",
      " \n",
      "Set1 : talk.religion.misc: sandvik god you in is that and to of the\n",
      "Set2 : talk.religion.misc: not it in you is and that to of the\n",
      "Commons : talk.religion.misc: you in is that and to of the\n",
      " \n"
     ]
    }
   ],
   "source": [
    "common_top10(classifier_fetch20_full, vectorizer_fetch20_full, newsgroups_train.target_names, classifier_fetch20_samples, vectorizer_fetch20_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that we cannot get much information from this rough analysis, filtering helps removing some non-meaningful data like email addresses but most of the remaining words are language basics as in the talk.politics.misc commonalities \"it is you and in that of to the\".\n",
    "We would need to create larger top sets, compare them and remove the words found in each topic, that would bring us the meaningul words specific to topics and not shared by the langue used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get cwl from all categories with top20 (cwl of all cwls)\n",
    "def global_common_top(n, classifier, vectorizer, categorizer):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    common_top = []\n",
    "    for i, category in enumerate(categorizer):\n",
    "        top = np.argsort(classifier.coef_[i])[-n:]\n",
    "        print(\"%s: %s: %s\" % (\"Classifier \", category, \" \".join(feature_names[top])))\n",
    "        if (len(common_top) == 0):\n",
    "            common_top = feature_names[top]\n",
    "        else:\n",
    "            common_top = cwl(common_top, feature_names[top])\n",
    "        print(\"%s: %s: %s\" % (\"Commons \", category, \" \".join(common_top)))\n",
    "        print(\"%s\" % (\" \")) # just for skipping a line between each category\n",
    "        \n",
    "    return common_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier : alt.atheism: they do for what have as god this be are not in and it you is that of to the\n",
      "Commons : alt.atheism: they do for what have as god this be are not in and it you is that of to the\n",
      " \n",
      "Classifier : comp.graphics: be thanks image or can any have this on that you in graphics it is for of and to the\n",
      "Commons : comp.graphics: for have this be in and it you is that of to the\n",
      " \n",
      "Classifier : comp.os.ms-windows.misc: but files this can on dos with have that in file of you for and is it to windows the\n",
      "Commons : comp.os.ms-windows.misc: for have this in and it you is that of to the\n",
      " \n",
      "Classifier : comp.sys.ibm.pc.hardware: controller bus this in my on card you that have with scsi for of drive is it and to the\n",
      "Commons : comp.sys.ibm.pc.hardware: for have this in and it you is that of to the\n",
      " \n",
      "Classifier : comp.sys.mac.hardware: can drive my if this on have in you with that apple for of mac it and is to the\n",
      "Commons : comp.sys.mac.hardware: for have this in and it you is that of to the\n",
      " \n",
      "Classifier : comp.windows.x: be have with an can on that you server motif for this it in of is and window to the\n",
      "Commons : comp.windows.x: for have this in and it you is that of to the\n",
      " \n",
      "Classifier : misc.forsale: please me is have new condition with you it of or in shipping offer 00 to and sale the for\n",
      "Commons : misc.forsale: for have in and it you is of to the\n",
      " \n",
      "Classifier : rec.autos: this with are was cars they my have for on is that in it of you and to car the\n",
      "Commons : rec.autos: for have in and it you is of to the\n",
      " \n",
      "Classifier : rec.motorcycles: me your this dod have with was on is my for that in of you it and bike to the\n",
      "Commons : rec.motorcycles: for have in and it you is of to the\n",
      " \n",
      "Classifier : rec.sport.baseball: be at you it baseball team have for his they year was is that of in and to he the\n",
      "Commons : rec.sport.baseball: for have in and it you is of to the\n",
      " \n",
      "Classifier : rec.sport.hockey: players play be on they for it you is was hockey team that game of he and in to the\n",
      "Commons : rec.sport.hockey: for in and it you is of to the\n",
      " \n",
      "Classifier : sci.crypt: on not as chip they for you clipper encryption this in be it is that key and of to the\n",
      "Commons : sci.crypt: for in and it you is of to the\n",
      " \n",
      "Classifier : sci.electronics: an there with or be if have are this on that for in it you is and of to the\n",
      "Commons : sci.electronics: for in and it you is of to the\n",
      " \n",
      "Classifier : sci.med: but msg as have with my not for be are this you that in it and is to of the\n",
      "Commons : sci.med: for in and it you is of to the\n",
      " \n",
      "Classifier : sci.space: would they are as this nasa was on you be for that it is in and space of to the\n",
      "Commons : sci.space: for in and it you is of to the\n",
      " \n",
      "Classifier : soc.religion.christian: was jesus for as are this be he we not you it in god and is that to of the\n",
      "Commons : soc.religion.christian: for in and it you is of to the\n",
      " \n",
      "Classifier : talk.politics.guns: guns be are as was not have for this they it gun is you in and that of to the\n",
      "Commons : talk.politics.guns: for in and it you is of to the\n",
      " \n",
      "Classifier : talk.politics.mideast: for jews were as was by they israeli are not it is israel that you in and to of the\n",
      "Commons : talk.politics.mideast: for in and it you is of to the\n",
      " \n",
      "Classifier : talk.politics.misc: was people on we have they as for not this are it is you in and that of to the\n",
      "Commons : talk.politics.misc: for in and it you is of to the\n",
      " \n",
      "Classifier : talk.religion.misc: for they was jesus this as be are he god not it in you is and that to of the\n",
      "Commons : talk.religion.misc: for in and it you is of to the\n",
      " \n",
      "[u'for', u'in', u'and', u'it', u'you', u'is', u'of', u'to', u'the']\n"
     ]
    }
   ],
   "source": [
    "commons_top20 = global_common_top(20, classifier_fetch20_samples, vectorizer_fetch20_samples, corpus_fetch20.target_names)\n",
    "pprint(commons_top20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the application of the global common to all categories top20 is below:\n",
    "we go from \"for have this be in and it you is that of to the\" after 2 categories to \"for in and it you is of to the\" at the end of all categories.\n",
    "We can see that some very common terms have still been discarded like \"be\", or \"this\". We should extend the top list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier : alt.atheism: islam which some just say with about religion would who by all there think no atheism can don one on he people or was we an so your but if they do for what have as god this be are not in and it you is that of to the\n",
      "Commons : alt.atheism: islam which some just say with about religion would who by all there think no atheism can don one on he people or was we an so your but if they do for what have as god this be are not in and it you is that of to the\n",
      " \n",
      "Classifier : comp.graphics: ftp do out some about software need hi at please does looking not am format 3d from as know anyone would me program file are there files with but if be thanks image or can any have this on that you in graphics it is for of and to the\n",
      "Commons : comp.graphics: some with about would there can on or but if do for have as this be are not in and it you is that of to the\n",
      " \n",
      "Classifier : comp.os.ms-windows.misc: am does cica at get as version know me anyone from will using ftp program problem are card any ax thanks not be use if drivers there driver my or but files this can on dos with have that in file of you for and is it to windows the\n",
      "Commons : comp.os.ms-windows.misc: with there can on or but if for have as this be are not in and it you is that of to the\n",
      " \n",
      "Classifier : comp.sys.ibm.pc.hardware: all dos one what get does there at do me isa drives has monitor would an system as are disk but not thanks pc be if any or ide can controller bus this in my on card you that have with scsi for of drive is it and to the\n",
      "Commons : comp.sys.ibm.pc.hardware: with there can on or but if for have as this be are not in and it you is that of to the\n",
      " \n",
      "Classifier : comp.sys.mac.hardware: se they centris lc about at use know has monitor does anyone would get quadra me from there simms any one are problem thanks what an not or but be can drive my if this on have in you with that apple for of mac it and is to the\n",
      "Commons : comp.sys.mac.hardware: with there can on or but if for have this be are not in and it you is that of to the\n",
      " \n",
      "Classifier : comp.windows.x: hi file sun please code am me mit display get there from program windows using xterm not as but x11r5 are application use my do thanks or widget if any be have with an can on that you server motif for this it in of is and window to the\n",
      "Commons : comp.windows.x: with there can on or but if for have this be are not in and it you is that of to the\n",
      " \n",
      "Classifier : misc.forsale: call an 25 on 50 your brand looking includes one best card 10 excellent used edu am this will mail at drive are all interested if asking email sell price please me is have new condition with you it of or in shipping offer 00 to and sale the for\n",
      "Commons : misc.forsale: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : rec.autos: ford new good so has from dealer get do all had just an as engine there at me would like can any your out but or not be about if this with are was cars they my have for on is that in it of you and to car the\n",
      "Commons : rec.autos: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : rec.motorcycles: bmw from so get out he any had at what can just helmet riding are like there do they about be not motorcycle if ride as one but bikes or me your this dod have with was on is my for that in of you it and bike to the\n",
      "Commons : rec.motorcycles: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : rec.sport.baseball: would had has one will all so out can who or him up think braves players last pitching hit with not runs are as if games on but this game be at you it baseball team have for his they year was is that of in and to he the\n",
      "Commons : rec.sport.baseball: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : rec.sport.hockey: think pittsburgh him or one playoffs what who leafs teams go has league detroit year not would if his at are with as this will but have nhl games season players play be on they for it you is was hockey team that game of he and in to the\n",
      "Commons : rec.sport.hockey: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : sci.crypt: some algorithm one all what do about has people security use from system escrow at but we your by nsa would or with have will if are can government keys on not as chip they for you clipper encryption this in be it is that key and of to the\n",
      "Commons : sci.crypt: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : sci.electronics: voltage your me some used out up my them get about know does like from what was circuit power anyone use not any one as they but at would can an there with or be if have are this on that for in it you is and of to the\n",
      "Commons : sci.electronics: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : sci.med: disease some an too one what has she your edu surrender they intellect shameful skepticism cadre there can dsl banks chastity n3jxp about pitt gordon geb if was on or but msg as have with my not for be are this you that in it and is to of the\n",
      "Commons : sci.med: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : sci.space: get which do more just lunar earth one what like shuttle an some we will about with can moon by launch orbit there not from but if or at have would they are as this nasa was on you be for that it is in and space of to the\n",
      "Commons : sci.space: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : soc.religion.christian: our your faith christian christ from about bible all one will people can christians there my or what do would they on by church with if who his but have was jesus for as are this be he we not you it in god and is that to of the\n",
      "Commons : soc.religion.christian: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : talk.politics.guns: so fire do one can who more them but there fbi don an about your their weapons from by no what we all with or were people on if would guns be are as was not have for this they it gun is you in and that of to the\n",
      "Commons : talk.politics.guns: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : talk.politics.mideast: his about if them no but one do all would an or there we with he arab what who your people have turkish from be armenians their on armenian this for jews were as was by they israeli are not it is israel that you in and to of the\n",
      "Commons : talk.politics.mideast: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : talk.politics.misc: just think clinton at one were there will tax no he their from so don by your all more do would who or if about government but what with be was people on we have they as for not this are it is you in and that of to the\n",
      "Commons : talk.politics.misc: with on or if for have this are in and it you is of to the\n",
      " \n",
      "Classifier : talk.religion.misc: an one bible out would christians about no me from there so were people my all do on christian if we his or by who what your with but have for they was jesus this as be are he god not it in you is and that to of the\n",
      "Commons : talk.religion.misc: with on or if for have this are in and it you is of to the\n",
      " \n",
      "[u'with',\n",
      " u'on',\n",
      " u'or',\n",
      " u'if',\n",
      " u'for',\n",
      " u'have',\n",
      " u'this',\n",
      " u'are',\n",
      " u'in',\n",
      " u'and',\n",
      " u'it',\n",
      " u'you',\n",
      " u'is',\n",
      " u'of',\n",
      " u'to',\n",
      " u'the']\n"
     ]
    }
   ],
   "source": [
    "commons_top50 = global_common_top(50, classifier_fetch20_samples, vectorizer_fetch20_samples, corpus_fetch20.target_names)\n",
    "pprint(commons_top50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending to a top50 we reach the following common terms: \"with on or if for have this are in and it you is of to the\".\n",
    "Between top20 and top50 were added \"with on or if have this are\" which are common terms that should be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# different words in 2 lists, with list comprehension (faster for big lists)\n",
    "def dwl(list1, list2):\n",
    "    return([x for x in (set(list1)^set(list2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['four', 'three']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1= [\"one\", \"two\", \"three\"]\n",
    "list2= [\"two\", \"one\", \"four\"]\n",
    "list3=dwl(list1, list2)\n",
    "list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for each filtered result, get top20 that are =/ from global cwl defined as commons_top50 above\n",
    "def extracted_common_top(n, classifier, vectorizer, categorizer, commons):\n",
    "    feature_names = np.asarray(vectorizer.get_feature_names())\n",
    "    for i, category in enumerate(categorizer):\n",
    "        top = np.argsort(classifier.coef_[i])[-n:]\n",
    "        print(\"%s: %s\" % (category, \" \".join(dwl(feature_names[top], commons))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alt.atheism: do they be as not with if on what that god or\n",
      "comp.graphics: be that graphics are with any if image can thanks\n",
      "comp.os.ms-windows.misc: files file that but are if windows can dos or\n",
      "comp.sys.ibm.pc.hardware: that bus controller are card if drive scsi my or\n",
      "comp.sys.mac.hardware: apple that mac are drive can my or\n",
      "comp.windows.x: be motif that an are if server window can or\n",
      "misc.forsale: offer are condition if me on 00 this please sale shipping new\n",
      "rec.autos: that cars they if was car my or\n",
      "rec.motorcycles: that are your if me my dod bike was or\n",
      "rec.sport.baseball: be his they that baseball are year with if on this team he was or at\n",
      "rec.sport.hockey: be play they that game are have with if this players hockey team he was or\n",
      "sci.crypt: be key that as are have not with if clipper they encryption or chip\n",
      "sci.electronics: be that an there\n",
      "sci.med: be that but as not if on msg my or\n",
      "sci.space: be they that as have with if would space nasa was or\n",
      "soc.religion.christian: be we that jesus as have not with if on god he was or\n",
      "talk.politics.guns: be they that as not with if on gun guns was or\n",
      "talk.politics.mideast: israel jews that as have not with by if on israeli this they were was or\n",
      "talk.politics.misc: we people as they not with if that was or\n",
      "talk.religion.misc: be they that jesus as have not with if on god he was or\n"
     ]
    }
   ],
   "source": [
    "extracted_common_top(20, classifier_fetch20_samples, vectorizer_fetch20_samples, corpus_fetch20.target_names, commons_top50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original top 10 : sci.space: for that it is in and space of to the  \n",
    "filtered top 20 : sci.space: be they that as have with if would space nasa was or  \n",
    "\n",
    "As can be seen the filtered top 20 is 2 words longer but much more significant even if some words are still very generic.  \n",
    "We will see in fextraction2 (next experience file in this batch) how to reach the same (and even much better) results with inverse document frequency vectorizer (we used it without any parameter here, which is not different than a CountVectorizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next to do:\n",
    "1. study overfit vs. performance vs. f1 scores\n",
    "2. work with tf-idf to improve results, compare the results with the post-classifier treatment discussed above (removing global commons) (tf-idf was used for the vectorizer already in this example file we need to compare with simple tf)\n",
    "3. study results between one big classifier (whole set) and multiple small classifiers merged together (probably less overfit but F1 score should be looked at) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
